<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>STA 3311 Sports Analytics II - 13&nbsp; Clustering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./14_dimension.html" rel="next">
<link href="./12_knn.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><link rel="stylesheet" href="webex.css">
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./13_clustering.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./"></a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Intro to Data Manipulation with <code>dplyr</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_Stringr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">String, Dates, and Joins</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_Tidymodels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction to Tidymodels</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_Regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Regression with Regularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_Logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Logistic Regression with Regularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_Mixed.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Mixed Effects Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_Data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Obtaining Sport Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_Web.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Web Scraping with Rvest</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_ML.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Introduction to Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10_Random.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Random Forests</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11_Spatial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Visualization of Player Tracking Data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12_knn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">K-Nearest Neighbors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13_clustering.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14_dimension.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Dimensionality Reduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15_SVM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16_DBSCAN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">DBSCAN</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17_MLP.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Multi-Layer Perceptron</span></span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#nba-player-styles" id="toc-nba-player-styles" class="nav-link active" data-scroll-target="#nba-player-styles"><span class="header-section-number">13.1</span> NBA Player Styles</a></li>
  <li>
<a href="#k-means-clustering" id="toc-k-means-clustering" class="nav-link" data-scroll-target="#k-means-clustering"><span class="header-section-number">13.2</span> K-means Clustering</a>
  <ul class="collapse">
<li><a href="#how-k-means-works" id="toc-how-k-means-works" class="nav-link" data-scroll-target="#how-k-means-works"><span class="header-section-number">13.2.1</span> How K-means Works</a></li>
  <li><a href="#example-with-two-features" id="toc-example-with-two-features" class="nav-link" data-scroll-target="#example-with-two-features"><span class="header-section-number">13.2.2</span> Example with two features</a></li>
  </ul>
</li>
  <li><a href="#choosing-the-number-of-clusters" id="toc-choosing-the-number-of-clusters" class="nav-link" data-scroll-target="#choosing-the-number-of-clusters"><span class="header-section-number">13.3</span> Choosing the Number of Clusters</a></li>
  <li><a href="#applying-k-means-to-nba-player-data" id="toc-applying-k-means-to-nba-player-data" class="nav-link" data-scroll-target="#applying-k-means-to-nba-player-data"><span class="header-section-number">13.4</span> Applying K-means to NBA Player Data</a></li>
  <li><a href="#interpreting-the-clusters" id="toc-interpreting-the-clusters" class="nav-link" data-scroll-target="#interpreting-the-clusters"><span class="header-section-number">13.5</span> Interpreting the Clusters</a></li>
  <li><a href="#visualizing-the-results" id="toc-visualizing-the-results" class="nav-link" data-scroll-target="#visualizing-the-results"><span class="header-section-number">13.6</span> Visualizing the Results</a></li>
  <li>
<a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering"><span class="header-section-number">13.7</span> Hierarchical Clustering</a>
  <ul class="collapse">
<li><a href="#the-basic-idea" id="toc-the-basic-idea" class="nav-link" data-scroll-target="#the-basic-idea"><span class="header-section-number">13.7.1</span> The Basic Idea</a></li>
  <li><a href="#distance-metrics-and-linkage-methods" id="toc-distance-metrics-and-linkage-methods" class="nav-link" data-scroll-target="#distance-metrics-and-linkage-methods"><span class="header-section-number">13.7.2</span> Distance Metrics and Linkage Methods</a></li>
  <li><a href="#applying-hierarchical-clustering-to-nba-player-data" id="toc-applying-hierarchical-clustering-to-nba-player-data" class="nav-link" data-scroll-target="#applying-hierarchical-clustering-to-nba-player-data"><span class="header-section-number">13.7.3</span> Applying Hierarchical Clustering to NBA Player Data</a></li>
  <li><a href="#interpreting-and-comparing-results" id="toc-interpreting-and-comparing-results" class="nav-link" data-scroll-target="#interpreting-and-comparing-results"><span class="header-section-number">13.7.4</span> Interpreting and Comparing Results</a></li>
  <li><a href="#advantages-and-limitations" id="toc-advantages-and-limitations" class="nav-link" data-scroll-target="#advantages-and-limitations"><span class="header-section-number">13.7.5</span> Advantages and Limitations</a></li>
  </ul>
</li>
  <li>
<a href="#comparing-k-means-and-hierarchical-clustering" id="toc-comparing-k-means-and-hierarchical-clustering" class="nav-link" data-scroll-target="#comparing-k-means-and-hierarchical-clustering"><span class="header-section-number">13.8</span> Comparing K-means and Hierarchical Clustering</a>
  <ul class="collapse">
<li><a href="#conceptual-differences" id="toc-conceptual-differences" class="nav-link" data-scroll-target="#conceptual-differences"><span class="header-section-number">13.8.1</span> Conceptual Differences</a></li>
  <li><a href="#comparing-cluster-assignments" id="toc-comparing-cluster-assignments" class="nav-link" data-scroll-target="#comparing-cluster-assignments"><span class="header-section-number">13.8.2</span> Comparing Cluster Assignments</a></li>
  <li><a href="#visual-comparison" id="toc-visual-comparison" class="nav-link" data-scroll-target="#visual-comparison"><span class="header-section-number">13.8.3</span> Visual Comparison</a></li>
  <li><a href="#statistical-comparison" id="toc-statistical-comparison" class="nav-link" data-scroll-target="#statistical-comparison"><span class="header-section-number">13.8.4</span> Statistical Comparison</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title">
<span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Clustering</span>
</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><blockquote class="blockquote">
<p>“Some days are diamonds. Some days are stones. Sometimes you have a couple of stones in a row.” - Gordon Hayward</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="pics/hayward.jpg" class="img-fluid figure-img" width="200"></p>
<figcaption class="figure-caption">With time expiring in the 2010 NCAA National Championship game, Gordon Hayward of Butler takes a half-court shot down 61-59 to Duke. The shot misses after hitting the backboard and rim, giving Duke the National Championship.</figcaption></figure>
</div>
<p>Clustering is a foundational technique in unsupervised machine learning, designed to uncover structure in datasets where no predefined labels or outcomes exist.</p>
<p>In contrast to supervised learning methods—where the goal is to predict a known response variable—clustering seeks to identify patterns or groupings in the data based solely on the similarity between observations. This makes it especially valuable in exploratory data analysis, where we are often interested in discovering hidden subgroups or natural partitions within a dataset.</p>
<p>The fundamental idea behind clustering is to group observations in such a way that those within the same cluster are more similar to each other than to those in different clusters. This similarity is typically measured using some form of distance or dissimilarity metric, with Euclidean distance being the most commonly used in practice.</p>
<p>In sports analytics, clustering can help identify different types of athletes, team strategies, or performance profiles without needing to label or classify the data in advance. For example, we might group NBA players based on their average points, assists, and rebounds per game to identify clusters such as high-volume scorers, versatile playmakers, or defensive specialists.</p>
<p>One of the most widely used clustering methods is <strong>K-means clustering</strong>, which begins by selecting a predetermined number of clusters, denoted by <em>k</em>.</p>
<p>The algorithm assigns each observation to the nearest cluster center and then updates the cluster centers based on the current assignments. This process continues iteratively until the cluster assignments stabilize, typically by minimizing a measure called the within-cluster sum of squares (WCSS).</p>
<p>K-means is relatively fast and works well when clusters are roughly spherical and of similar size, but it does require the analyst to specify the number of clusters in advance.</p>
<p>Another important approach is <strong>hierarchical clustering</strong>, which does not require the number of clusters to be specified up front. Instead, it builds a hierarchy or tree of clusters—called a <strong>dendrogram</strong>—by either successively merging smaller clusters into larger ones (agglomerative) or splitting a large cluster into smaller ones (divisive). This method is particularly useful for understanding the nested structure of the data and for visualizing how clusters are related to one another at various levels of similarity.</p>
<p>In sports settings, hierarchical clustering can be used to explore how players or teams group together at different levels of granularity, which can help coaches, analysts, or general managers make more informed decisions.</p>
<p>Before applying any clustering technique, it is often essential to preprocess the data by standardizing the variables. This is especially true when the variables are on different scales—for instance, points per game might range from 0 to 30, while blocks per game rarely exceed 3. Without standardization, variables with larger magnitudes can disproportionately influence the clustering results, potentially leading to misleading groupings.</p>
<p>Clustering is not just a mechanical process of dividing data into groups. Its real power lies in the interpretation and context-specific meaning of the clusters. In sports analytics, once clusters are formed, analysts must interpret what each group represents. Are they reflecting different player roles, team strategies, or developmental stages? The meaning of a cluster depends entirely on the variables used and the context in which the data is collected. This interpretive step is crucial for turning the output of a clustering algorithm into actionable insight.</p>
<p>In the sections that follow, we will apply clustering techniques to sports datasets and demonstrate how they can be used to uncover playing styles and performance archetypes. We will begin with K-means clustering applied to NBA player statistics and then explore hierarchical clustering as an alternative method. Along the way, we will discuss how to evaluate the quality of clusters and how to visualize them for deeper understanding.</p>
<section id="nba-player-styles" class="level2" data-number="13.1"><h2 data-number="13.1" class="anchored" data-anchor-id="nba-player-styles">
<span class="header-section-number">13.1</span> NBA Player Styles</h2>
<p>To demonstrate the power of clustering in a sports context, we begin with a case study using player statistics from the NBA. Our goal is to group players based on their in-game performance during a single season, allowing us to identify archetypes such as volume scorers, defensive anchors, or well-rounded contributors. By clustering players based on key statistical measures, we can uncover patterns that may not be obvious from traditional per-game summaries.</p>
<p>In this case study, we use the <code>hoopR</code> package in R, which provides access to rich and detailed NBA player box score data via the ESPN API. Specifically, we’ll use data from the 2021–2022 season, focusing on player-level averages across several core performance metrics.</p>
<p>We begin by loading the necessary libraries and retrieving the data. The <code>tidymodels</code> ecosystem helps us structure our data processing and modeling workflow, while <code>hoopR</code> provides the raw game-level data.</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/sportsdataverse/hoopR">hoopR</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code><a href="https://hoopR.sportsdataverse.org/reference/load_nba_player_box.html">load_nba_player_box()</a></code> function fetches player box score data for the season. We then clean and summarize the data to compute per-game averages for each player. These averages form the basis of our clustering analysis.</p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/sfirke/janitor">janitor</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get NBA player box scores for the 2021–2022 season</span></span>
<span><span class="va">nba_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://hoopR.sportsdataverse.org/reference/load_nba_player_box.html">load_nba_player_box</a></span><span class="op">(</span>season <span class="op">=</span> <span class="fl">2022</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The raw box score data includes one row per player per game. Since we are interested in clustering based on overall performance style rather than single-game variation, we aggregate the data to the player level. We compute average points (PTS), assists (AST), rebounds (REB), steals (STL), blocks (BLK), turnovers (TOV), three-pointers made (FG3M), field goal attempts (FGA), and free throw attempts (FTA), and restrict our analysis to players who appeared in at least 30 games during the season to ensure we are analyzing a stable sample.</p>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">player_stats</span> <span class="op">&lt;-</span> <span class="va">nba_data</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">athlete_display_name</span>, <span class="va">team_abbreviation</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span></span>
<span>    games <span class="op">=</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/context.html">n</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>    pts <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">points</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>    ast <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">assists</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>    reb <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">rebounds</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>    stl <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">steals</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>    blk <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">blocks</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>    tov <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">turnovers</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>    fg3m <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">three_point_field_goals_made</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>    ftm <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">free_throws_made</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>    position <span class="op">=</span> <span class="fu">collapse</span><span class="fu">::</span><span class="fu"><a href="https://sebkrantz.github.io/collapse/reference/fmode.html">fmode</a></span><span class="op">(</span><span class="va">athlete_position_abbreviation</span><span class="op">)</span>,</span>
<span>    .groups <span class="op">=</span> <span class="st">"drop"</span></span>
<span>  <span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">games</span> <span class="op">&gt;=</span> <span class="fl">30</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Before performing clustering, we standardize the numeric features. This is a crucial step because the variables we are using are on different scales. For instance, points per game typically ranges from single digits to the low 30s, while steals or blocks per game rarely exceed 2 or 3. Without scaling, clustering algorithms like K-means will be unduly influenced by the variables with larger numeric ranges.</p>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Scale the numeric columns</span></span>
<span><span class="va">scaled_stats</span> <span class="op">&lt;-</span> <span class="fu">recipe</span><span class="op">(</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">player_stats</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">step_rm</span><span class="op">(</span><span class="va">athlete_display_name</span>, <span class="va">team_abbreviation</span>, <span class="va">position</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">step_normalize</span><span class="op">(</span><span class="fu">all_numeric_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu">prep</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">dat</span> <span class="op">=</span> <span class="va">scaled_stats</span> <span class="op">|&gt;</span> <span class="fu">bake</span><span class="op">(</span>new_data<span class="op">=</span><span class="cn">NULL</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The goal of this clustering exercise is not just to segment players for the sake of grouping, but to discover meaningful player types. For example, we may find a cluster of players who attempt many field goals and score efficiently, representing primary scorers. Another group may consist of players with high assist and rebound numbers, pointing to versatile point-forwards or playmakers. Defensive specialists may emerge as a group with high block and steal rates but low scoring output.</p>
</section><section id="k-means-clustering" class="level2" data-number="13.2"><h2 data-number="13.2" class="anchored" data-anchor-id="k-means-clustering">
<span class="header-section-number">13.2</span> K-means Clustering</h2>
<p>Now that we have prepared our NBA player statistics data, the next step is to apply <strong>K-means clustering</strong>, one of the most widely used and intuitive clustering algorithms. K-means aims to partition the observations into a fixed number of groups, or <strong>clusters</strong>, such that each observation belongs to the cluster with the nearest <strong>centroid</strong>—the mean position of all points within that cluster.</p>
<section id="how-k-means-works" class="level3" data-number="13.2.1"><h3 data-number="13.2.1" class="anchored" data-anchor-id="how-k-means-works">
<span class="header-section-number">13.2.1</span> How K-means Works</h3>
<p>K-means clustering is an iterative algorithm that seeks to minimize the total <strong>within-cluster sum of squares (WCSS)</strong>, which measures the variance of the observations around the cluster centroids. Here’s how the algorithm works conceptually:</p>
<ol type="1">
<li>Choose the number of clusters, <span class="math inline">\(k\)</span>, to create.</li>
<li>Randomly assign initial positions for the <span class="math inline">\(k\)</span> centroids.</li>
<li>Assign each observation to the cluster whose centroid is closest, based on Euclidean distance.</li>
<li>Recompute the centroids as the mean of the observations in each cluster.</li>
<li>Repeat steps 3 and 4 until the assignments stop changing or a maximum number of iterations is reached.</li>
</ol>
<p>The algorithm always converges, but it may converge to a <strong>local minimum</strong> depending on the starting positions. Therefore, it is common practice to run the algorithm multiple times with different random starts and retain the best result.</p>
</section><section id="example-with-two-features" class="level3" data-number="13.2.2"><h3 data-number="13.2.2" class="anchored" data-anchor-id="example-with-two-features">
<span class="header-section-number">13.2.2</span> Example with two features</h3>
<p>Suppose we only looked at a sample of 30 players with the features <code>reb</code> and <code>ast</code>.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>We can see what appears to be three grouping of players:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>How do we get an unsupervised method to determine these clusters instead of us visually determining them?</p>
<p>We can start by specifying that there will be three cluster and putting three points at random on the plot. We will call these points the centroids. They are the orange points in the plot below.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>We now find the distance from each ppoint to the closest centroid.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>We now cluster those points that share a centroid (same color in the plot below) and then update the centroids by calculating the middle of the points in the cluster.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>We now repeat the process. That is, we determine which centroid is closest to each point.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>Next, color code the points based on the closest centroid and recalculate the position of the centroid.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>This process continues until the centroids no longer moves.</p>
<p>With more than two features, the idea is still the same: calculate the distance to each centroid and make clusters based on those that are closest. Then we update the centroids and repeat the process. The difference with higher number of features is that we can no longer visualize the process as we have just done here.</p>
</section></section><section id="choosing-the-number-of-clusters" class="level2" data-number="13.3"><h2 data-number="13.3" class="anchored" data-anchor-id="choosing-the-number-of-clusters">
<span class="header-section-number">13.3</span> Choosing the Number of Clusters</h2>
<p>One of the most important decisions in K-means clustering is selecting an appropriate value for <span class="math inline">\(k\)</span>, the number of clusters. Too few clusters can lead to overly broad groupings that obscure important differences, while too many clusters may produce noise or overfit to small nuances in the data.</p>
<p>A commonly used heuristic is the <strong>Elbow Method</strong>, which plots the total WCSS for different values of <span class="math inline">\(k\)</span>. The idea is to choose the number of clusters at which the rate of decrease in WCSS sharply slows down—resembling an “elbow” in the plot. This point represents a good trade-off between reducing within-cluster variance and avoiding overly complex models.</p>
<p>A handy plot for doing this is the <code>fviz_nbclust</code> plot in the <code>factoextra</code> package. Below, we examine the plot for all the players but with just the features <code>reb</code> and <code>ast</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.sthda.com/english/rpkgs/factoextra">factoextra</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_nbclust.html">fviz_nbclust</a></span><span class="op">(</span><span class="va">dat</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">reb</span>, <span class="va">ast</span><span class="op">)</span> , <span class="va">kmeans</span>, method <span class="op">=</span> <span class="st">"wss"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>Here we see the decrees in WCSS starts to level off at <span class="math inline">\(k=5\)</span>. This suggests that a five-cluster solution captures most of the structure in the data without unnecessary complexity.</p>
</section><section id="applying-k-means-to-nba-player-data" class="level2" data-number="13.4"><h2 data-number="13.4" class="anchored" data-anchor-id="applying-k-means-to-nba-player-data">
<span class="header-section-number">13.4</span> Applying K-means to NBA Player Data</h2>
<p>Let’s first determine the number of clusters we should use when using all 9 features.</p>
<div class="cell">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_nbclust.html">fviz_nbclust</a></span><span class="op">(</span><span class="va">dat</span>, <span class="va">kmeans</span>, method <span class="op">=</span> <span class="st">"wss"</span>, k.max <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>In this situation, we see the number of knots is not easily chosen by the elbow method. The decrease in WCSS is consistent, for the most part, throughout the values of k.</p>
<p>Since there are traditionally five positions on a basketball team, let’s go with <span class="math inline">\(k=5\)</span>.</p>
<p>Once we have selected the number of clusters, we fit the K-means model using the <code><a href="https://rdrr.io/r/stats/kmeans.html">kmeans()</a></code> function in base R. We use the <code>nstart = 25</code> argument to perform the clustering 25 times with different initial centroids and keep the best solution based on the total WCSS.</p>
<div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">kmeans_fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/kmeans.html">kmeans</a></span><span class="op">(</span><span class="va">dat</span>, centers <span class="op">=</span> <span class="fl">5</span>, nstart <span class="op">=</span> <span class="fl">25</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>kmeans_fit</code> object contains several components, including the cluster assignments for each observation, the coordinates of the centroids, and the total within-cluster sum of squares. We add the cluster assignments to our original player-level dataset for further analysis.</p>
<div class="cell">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">player_stats</span> <span class="op">&lt;-</span> <span class="va">player_stats</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html">mutate</a></span><span class="op">(</span>cluster <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">kmeans_fit</span><span class="op">$</span><span class="va">cluster</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="interpreting-the-clusters" class="level2" data-number="13.5"><h2 data-number="13.5" class="anchored" data-anchor-id="interpreting-the-clusters">
<span class="header-section-number">13.5</span> Interpreting the Clusters</h2>
<p>The most meaningful part of a clustering analysis is the interpretation of the resulting groups. To do this, we examine the average values of the original variables within each cluster. This helps us understand what defines each group.</p>
<div class="cell">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">player_stats</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">cluster</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/across.html">across</a></span><span class="op">(</span><span class="va">pts</span><span class="op">:</span><span class="va">ftm</span>, <span class="va">mean</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 9
  cluster   pts   ast   reb   stl   blk   tov  fg3m   ftm
  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1 1       13.1  2.78   3.82 0.893 0.361 1.40  1.91  1.81 
2 2       22.2  6.28   6.39 1.21  0.516 3.03  2.10  4.32 
3 3        4.36 0.930  2.41 0.350 0.247 0.568 0.544 0.573
4 4       11.9  1.87   8.03 0.758 1.20  1.42  0.537 1.96 
5 5        6.05 1.27   2.95 0.535 0.310 0.704 0.705 0.836</code></pre>
</div>
</div>
<p>This summary can be used to determine similar players in each cluster. Let’s look at the distribution of positions for the second cluster which has the highest aveage points and assists.</p>
<div class="cell">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">player_stats</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">cluster</span> <span class="op">==</span> <span class="fl">2</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">position</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>position
 C  F PF PG SF SG 
 3  1  5 19  7 13 </code></pre>
</div>
</div>
<p>Most of these players in cluster 2 are guards. Let’s look at the players in this cluster.</p>
<div class="cell">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">player_stats</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">cluster</span><span class="op">==</span><span class="fl">2</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html">select</a></span><span class="op">(</span><span class="va">athlete_display_name</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 48 × 1
   athlete_display_name
   &lt;chr&gt;               
 1 Anthony Edwards     
 2 Bradley Beal        
 3 Brandon Ingram      
 4 CJ McCollum         
 5 Cade Cunningham     
 6 Chris Paul          
 7 Cole Anthony        
 8 D'Angelo Russell    
 9 Darius Garland      
10 De'Aaron Fox        
# ℹ 38 more rows</code></pre>
</div>
</div>
</section><section id="visualizing-the-results" class="level2" data-number="13.6"><h2 data-number="13.6" class="anchored" data-anchor-id="visualizing-the-results">
<span class="header-section-number">13.6</span> Visualizing the Results</h2>
<p>To help interpret the clusters visually, we can use the <code><a href="https://rdrr.io/pkg/factoextra/man/fviz_cluster.html">fviz_cluster()</a></code> function, which projects the high-dimensional data onto two principal components and shows the cluster memberships in two dimensions.</p>
<div class="cell">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_cluster.html">fviz_cluster</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">dat</span>, cluster <span class="op">=</span> <span class="va">kmeans_fit</span><span class="op">$</span><span class="va">cluster</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>While this visualization simplifies the data into two dimensions, it still provides a helpful overview of how well the clusters are separated and whether there is any substantial overlap between groups. Large, well-separated clusters suggest that the statistical profiles of the players are meaningfully different.</p>
</section><section id="hierarchical-clustering" class="level2" data-number="13.7"><h2 data-number="13.7" class="anchored" data-anchor-id="hierarchical-clustering">
<span class="header-section-number">13.7</span> Hierarchical Clustering</h2>
<p>While K-means clustering is powerful and widely used, it requires the analyst to choose the number of clusters in advance and is most effective when clusters are roughly spherical and of similar size. An alternative technique that overcomes some of these limitations is <strong>hierarchical clustering</strong>, which builds a tree-like structure of nested clusters. This method provides a flexible and informative view of the relationships among observations and is particularly helpful when we want to explore the data at multiple levels of granularity.</p>
<section id="the-basic-idea" class="level3" data-number="13.7.1"><h3 data-number="13.7.1" class="anchored" data-anchor-id="the-basic-idea">
<span class="header-section-number">13.7.1</span> The Basic Idea</h3>
<p>Hierarchical clustering operates by computing a measure of <strong>dissimilarity</strong> (or distance) between each pair of observations, then successively merging (or, in some cases, splitting) observations and clusters based on that measure. The result is a <strong>dendrogram</strong>—a tree diagram that illustrates how observations group together.</p>
<p>There are two primary types of hierarchical clustering:</p>
<ul>
<li>
<strong>Agglomerative clustering</strong>, which begins with each observation in its own cluster and repeatedly merges the two closest clusters.</li>
<li>
<strong>Divisive clustering</strong>, which starts with all observations in a single cluster and successively splits them.</li>
</ul>
<p>In practice, agglomerative clustering is far more common and is the default approach in most statistical software.</p>
</section><section id="distance-metrics-and-linkage-methods" class="level3" data-number="13.7.2"><h3 data-number="13.7.2" class="anchored" data-anchor-id="distance-metrics-and-linkage-methods">
<span class="header-section-number">13.7.2</span> Distance Metrics and Linkage Methods</h3>
<p>Hierarchical clustering requires two key decisions: how to measure the distance between observations, and how to define the distance between clusters.</p>
<p>The most common <strong>distance metric</strong> is <strong>Euclidean distance</strong>, which measures the straight-line distance between two points in multidimensional space. Other options include Manhattan distance and cosine similarity, though Euclidean is usually sufficient for standardized numerical data.</p>
<p>Once distances between individual observations are calculated, we must decide how to compute the distance between clusters. This is known as the <strong>linkage method</strong>, and several options are available:</p>
<ul>
<li>
<strong>Single linkage</strong>: the shortest distance between any two points in the two clusters.</li>
<li>
<strong>Complete linkage</strong>: the greatest distance between any two points.</li>
<li>
<strong>Average linkage</strong>: the average of all pairwise distances.</li>
<li>
<strong>Ward’s method</strong>: minimizes the total within-cluster variance and tends to produce compact, spherical clusters (similar in spirit to K-means).</li>
</ul>
<p>For our analysis of NBA player statistics, we will use <strong>Ward’s method</strong>, which is generally well-suited for quantitative data and tends to create clusters of similar size.</p>
</section><section id="applying-hierarchical-clustering-to-nba-player-data" class="level3" data-number="13.7.3"><h3 data-number="13.7.3" class="anchored" data-anchor-id="applying-hierarchical-clustering-to-nba-player-data">
<span class="header-section-number">13.7.3</span> Applying Hierarchical Clustering to NBA Player Data</h3>
<p>We begin by computing the distance matrix and fitting the hierarchical clustering model.</p>
<div class="cell">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Remove player names and compute Euclidean distances</span></span>
<span><span class="va">dist_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/dist.html">dist</a></span><span class="op">(</span><span class="va">dat</span> <span class="op">)</span></span>
<span></span>
<span><span class="co"># Perform hierarchical clustering using Ward's method</span></span>
<span><span class="va">hc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span><span class="op">(</span><span class="va">dist_matrix</span>, method <span class="op">=</span> <span class="st">"ward.D2"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code><a href="https://rdrr.io/r/stats/hclust.html">hclust()</a></code> function returns an object containing the hierarchy of merges, which we can visualize using a dendrogram.</p>
<div class="cell">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span><span class="va">hc</span>, k <span class="op">=</span> <span class="fl">5</span>, rect <span class="op">=</span> <span class="cn">TRUE</span>, labels_track_height <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>In the dendrogram, each leaf node represents an individual player. As we move up the tree, branches merge into larger clusters based on their similarity. The height at which two clusters are merged corresponds to the dissimilarity between them.</p>
<p>We can use this tree to cut the data into any number of clusters by drawing a horizontal line across the dendrogram. For example, if we cut the tree at <span class="math inline">\(k = 5\)</span>, we obtain four clusters similar to our K-means example.</p>
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cluster_assignments</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cutree.html">cutree</a></span><span class="op">(</span><span class="va">hc</span>, k <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Add cluster labels to the player data</span></span>
<span><span class="va">player_stats</span><span class="op">$</span><span class="va">hc_cluster</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">cluster_assignments</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="interpreting-and-comparing-results" class="level3" data-number="13.7.4"><h3 data-number="13.7.4" class="anchored" data-anchor-id="interpreting-and-comparing-results">
<span class="header-section-number">13.7.4</span> Interpreting and Comparing Results</h3>
<p>Once we have assigned players to clusters using hierarchical clustering, we can interpret the groups in the same way as we did with K-means. For example, we might summarize the mean statistics for each cluster:</p>
<div class="cell">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">player_stats</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">hc_cluster</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarise</a></span><span class="op">(</span><span class="fu"><a href="https://dplyr.tidyverse.org/reference/across.html">across</a></span><span class="op">(</span><span class="va">pts</span><span class="op">:</span><span class="va">ftm</span>, <span class="va">mean</span>, na.rm <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 9
  hc_cluster   pts   ast   reb   stl   blk   tov  fg3m   ftm
  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1 1           7.06  1.46  3.14 0.583 0.336 0.792 0.878 0.957
2 2           4.96  1.03  2.63 0.388 0.278 0.619 0.572 0.671
3 3          13.1   2.10  8.55 0.804 1.21  1.58  0.644 2.23 
4 4          14.5   2.84  3.99 0.876 0.341 1.47  2.12  2.06 
5 5          20.3   6.10  5.81 1.26  0.518 2.86  2.04  3.82 </code></pre>
</div>
</div>
<p>This output allows us to describe the average player profile for each cluster. It’s important to note that while hierarchical clustering and K-means may yield similar types of player groupings, they do not always agree. For example, hierarchical clustering may detect smaller subgroups or more gradual transitions between player types, since it is not constrained to fixed-size or spherical clusters.</p>
<p>We can also compare the cluster assignments from K-means and hierarchical clustering directly:</p>
<div class="cell">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>KMeans <span class="op">=</span> <span class="va">player_stats</span><span class="op">$</span><span class="va">cluster</span>, Hierarchical <span class="op">=</span> <span class="va">player_stats</span><span class="op">$</span><span class="va">hc_cluster</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Hierarchical
KMeans   1   2   3   4   5
     1  31   4   1  65  10
     2   0   0   4   1  43
     3   4 120   0   0   0
     4   6   5  37   0   1
     5 127  12   0   0   0</code></pre>
</div>
</div>
<p>This contingency table shows how much overlap exists between the two methods. A high degree of agreement suggests that the player groupings are robust and meaningful, while major differences may indicate that the choice of clustering method influences the interpretation.</p>
</section><section id="advantages-and-limitations" class="level3" data-number="13.7.5"><h3 data-number="13.7.5" class="anchored" data-anchor-id="advantages-and-limitations">
<span class="header-section-number">13.7.5</span> Advantages and Limitations</h3>
<p>One key advantage of hierarchical clustering is that it provides a complete picture of how observations relate to one another at all levels of similarity. This is particularly useful in exploratory settings where the number of natural groupings is unclear. It also does not require the analyst to pre-specify the number of clusters, unlike K-means.</p>
<p>However, hierarchical clustering has some limitations. It can be computationally expensive for large datasets, since it must compute and store the entire distance matrix. Additionally, once a merge is made, it cannot be undone, meaning that early decisions in the hierarchy can influence the final outcome even if better options become available later. As a result, hierarchical clustering is best suited for medium-sized datasets where interpretability is a primary concern.</p>
</section></section><section id="comparing-k-means-and-hierarchical-clustering" class="level2" data-number="13.8"><h2 data-number="13.8" class="anchored" data-anchor-id="comparing-k-means-and-hierarchical-clustering">
<span class="header-section-number">13.8</span> Comparing K-means and Hierarchical Clustering</h2>
<p>Clustering is often an exploratory process, and choosing the “best” algorithm depends on both the structure of the data and the goals of the analysis. In this section, we compare the results of <strong>K-means</strong> and <strong>hierarchical clustering</strong> applied to our NBA player statistics dataset. Our goal is not only to assess which algorithm performs better, but also to explore how the insights generated by each method can differ in practical and strategic ways.</p>
<section id="conceptual-differences" class="level3" data-number="13.8.1"><h3 data-number="13.8.1" class="anchored" data-anchor-id="conceptual-differences">
<span class="header-section-number">13.8.1</span> Conceptual Differences</h3>
<p>Before comparing the results quantitatively, it’s useful to revisit the conceptual differences between the two approaches.</p>
<p><strong>K-means</strong> is a <strong>partitioning</strong> method. It assumes that the data are divisible into a fixed number of clusters and tries to optimize the grouping by minimizing the total within-cluster variance. The clusters produced by K-means tend to be compact and evenly sized, which works well when the true groups are spherical and well-separated. However, the algorithm requires us to specify the number of clusters in advance and is sensitive to the initial placement of centroids.</p>
<p>In contrast, <strong>hierarchical clustering</strong> is a <strong>connectivity-based</strong> method. It does not require a predetermined number of clusters and instead builds a full hierarchy that shows relationships among observations at all levels of similarity. It can reveal nested or irregular groupings, and its results are typically presented as a dendrogram. This method offers more interpretive flexibility but can be computationally intensive and less effective when the dataset contains many noise points or overlapping clusters.</p>
</section><section id="comparing-cluster-assignments" class="level3" data-number="13.8.2"><h3 data-number="13.8.2" class="anchored" data-anchor-id="comparing-cluster-assignments">
<span class="header-section-number">13.8.2</span> Comparing Cluster Assignments</h3>
<p>To see how the two methods compare on our NBA player dataset, we can tabulate the cluster assignments for each player. Recall that we created a <code>cluster</code> variable from K-means and a <code>hc_cluster</code> variable from hierarchical clustering.</p>
<div class="cell">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>KMeans <span class="op">=</span> <span class="va">player_stats</span><span class="op">$</span><span class="va">cluster</span>, Hierarchical <span class="op">=</span> <span class="va">player_stats</span><span class="op">$</span><span class="va">hc_cluster</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Hierarchical
KMeans   1   2   3   4   5
     1  31   4   1  65  10
     2   0   0   4   1  43
     3   4 120   0   0   0
     4   6   5  37   0   1
     5 127  12   0   0   0</code></pre>
</div>
</div>
<p>This cross-tabulation shows how many players were assigned to each pair of K-means and hierarchical clusters. Large values along the diagonal suggest strong agreement between the methods, while high off-diagonal values indicate differences. For example, if most players in K-means Cluster 1 are also in hierarchical Cluster 3, we might consider these to represent similar player types despite the differing algorithmic foundations.</p>
<p>You may find that some clusters align fairly well, particularly those representing extreme player types—such as high-scoring, high-usage stars or low-usage defensive specialists. On the other hand, more nuanced or hybrid players may be assigned to different clusters by each method depending on how the algorithm interprets the multidimensional space.</p>
</section><section id="visual-comparison" class="level3" data-number="13.8.3"><h3 data-number="13.8.3" class="anchored" data-anchor-id="visual-comparison">
<span class="header-section-number">13.8.3</span> Visual Comparison</h3>
<p>Visualizing the clusters produced by each method is another effective way to compare results. We can again use the <code><a href="https://rdrr.io/pkg/factoextra/man/fviz_cluster.html">fviz_cluster()</a></code> function to project the data into two dimensions using principal component analysis and color-code the observations by their cluster membership.</p>
<div class="cell">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_cluster.html">fviz_cluster</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">dat</span>, cluster <span class="op">=</span> <span class="va">kmeans_fit</span><span class="op">$</span><span class="va">cluster</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"K-means Clustering of NBA Players"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-26-1.png" class="img-fluid" width="576"></p>
</div>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_dend.html">fviz_dend</a></span><span class="op">(</span><span class="va">hc</span>, k <span class="op">=</span> <span class="fl">5</span>, rect <span class="op">=</span> <span class="cn">TRUE</span>, labels_track_height <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"Hierarchical Clustering Dendrogram"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="13_clustering_files/figure-html/unnamed-chunk-26-2.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>These visualizations provide complementary views of the data. The K-means plot shows distinct, compact groups, while the dendrogram reveals the hierarchical structure—allowing us to see not only how players group together but also how those groups relate to one another.</p>
</section><section id="statistical-comparison" class="level3" data-number="13.8.4"><h3 data-number="13.8.4" class="anchored" data-anchor-id="statistical-comparison">
<span class="header-section-number">13.8.4</span> Statistical Comparison</h3>
<p>Although clustering is fundamentally an unsupervised technique, there are ways to assess and compare the quality of different clustering results. One popular metric is the <strong>Silhouette score</strong>, which measures how well each observation fits within its assigned cluster compared to other clusters.</p>
<p>We can compute the average silhouette width for both methods:</p>
<div class="cell">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Silhouette score for K-means</span></span>
<span><span class="va">sil_kmeans</span> <span class="op">&lt;-</span> <span class="fu">cluster</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/cluster/man/silhouette.html">silhouette</a></span><span class="op">(</span><span class="va">kmeans_fit</span><span class="op">$</span><span class="va">cluster</span>, <span class="va">dist_matrix</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">sil_kmeans</span><span class="op">[</span>, <span class="st">"sil_width"</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.2197183</code></pre>
</div>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Silhouette score for hierarchical clustering</span></span>
<span><span class="va">sil_hc</span> <span class="op">&lt;-</span> <span class="fu">cluster</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/cluster/man/silhouette.html">silhouette</a></span><span class="op">(</span><span class="va">cluster_assignments</span>, <span class="va">dist_matrix</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">sil_hc</span><span class="op">[</span>, <span class="st">"sil_width"</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.1827008</code></pre>
</div>
</div>
<p>Higher average silhouette scores indicate better-defined clusters. While this metric should not be the sole basis for choosing a method, it can provide a helpful indication of how well the structure discovered by each algorithm matches the natural groupings in the data.</p>


</section></section></main><!-- /main --><script>

/* update total correct if #webex-total_correct exists */
update_total_correct = function() {
  console.log("webex: update total_correct");

  var t = document.getElementsByClassName("webex-total_correct");
  for (var i = 0; i < t.length; i++) {
    p = t[i].parentElement;
    var correct = p.getElementsByClassName("webex-correct").length;
    var solvemes = p.getElementsByClassName("webex-solveme").length;
    var radiogroups = p.getElementsByClassName("webex-radiogroup").length;
    var selects = p.getElementsByClassName("webex-select").length;

    t[i].innerHTML = correct + " of " + (solvemes + radiogroups + selects) + " correct";
  }
}

/* webex-solution button toggling function */
b_func = function() {
  console.log("webex: toggle hide");

  var cl = this.parentElement.classList;
  if (cl.contains('open')) {
    cl.remove("open");
  } else {
    cl.add("open");
  }
}

/* check answers */
check_func = function() {
  console.log("webex: check answers");

  var cl = this.parentElement.classList;
  if (cl.contains('unchecked')) {
    cl.remove("unchecked");
    this.innerHTML = "Hide Answers";
  } else {
    cl.add("unchecked");
    this.innerHTML = "Show Answers";
  }
}

/* function for checking solveme answers */
solveme_func = function(e) {
  console.log("webex: check solveme");

  var real_answers = JSON.parse(this.dataset.answer);
  var my_answer = this.value;
  var cl = this.classList;
  if (cl.contains("ignorecase")) {
    my_answer = my_answer.toLowerCase();
  }
  if (cl.contains("nospaces")) {
    my_answer = my_answer.replace(/ /g, "")
  }

  if (my_answer == "") {
    cl.remove("webex-correct");
    cl.remove("webex-incorrect");
  } else if (real_answers.includes(my_answer)) {
    cl.add("webex-correct");
    cl.remove("webex-incorrect");
  } else {
    cl.add("webex-incorrect");
    cl.remove("webex-correct");
  }

  // match numeric answers within a specified tolerance
  if(this.dataset.tol > 0){
    var tol = JSON.parse(this.dataset.tol);
    var matches = real_answers.map(x => Math.abs(x - my_answer) < tol)
    if (matches.reduce((a, b) => a + b, 0) > 0) {
      cl.add("webex-correct");
    } else {
      cl.remove("webex-correct");
    }
  }

  // added regex bit
  if (cl.contains("regex")){
    answer_regex = RegExp(real_answers.join("|"))
    if (answer_regex.test(my_answer)) {
      cl.add("webex-correct");
    }
  }

  update_total_correct();
}

/* function for checking select answers */
select_func = function(e) {
  console.log("webex: check select");

  var cl = this.classList

  /* add style */
  cl.remove("webex-incorrect");
  cl.remove("webex-correct");
  if (this.value == "answer") {
    cl.add("webex-correct");
  } else if (this.value != "blank") {
    cl.add("webex-incorrect");
  }

  update_total_correct();
}

/* function for checking radiogroups answers */
radiogroups_func = function(e) {
  console.log("webex: check radiogroups");

  var checked_button = document.querySelector('input[name=' + this.id + ']:checked');
  var cl = checked_button.parentElement.classList;
  var labels = checked_button.parentElement.parentElement.children;

  /* get rid of styles */
  for (i = 0; i < labels.length; i++) {
    labels[i].classList.remove("webex-incorrect");
    labels[i].classList.remove("webex-correct");
  }

  /* add style */
  if (checked_button.value == "answer") {
    cl.add("webex-correct");
  } else {
    cl.add("webex-incorrect");
  }

  update_total_correct();
}

window.onload = function() {
  console.log("webex onload");
  /* set up solution buttons */
  var buttons = document.getElementsByTagName("button");

  for (var i = 0; i < buttons.length; i++) {
    if (buttons[i].parentElement.classList.contains('webex-solution')) {
      buttons[i].onclick = b_func;
    }
  }

  var check_sections = document.getElementsByClassName("webex-check");
  console.log("check:", check_sections.length);
  for (var i = 0; i < check_sections.length; i++) {
    check_sections[i].classList.add("unchecked");

    let btn = document.createElement("button");
    btn.innerHTML = "Show Answers";
    btn.classList.add("webex-check-button");
    btn.onclick = check_func;
    check_sections[i].appendChild(btn);

    let spn = document.createElement("span");
    spn.classList.add("webex-total_correct");
    check_sections[i].appendChild(spn);
  }

  /* set up webex-solveme inputs */
  var solveme = document.getElementsByClassName("webex-solveme");

  for (var i = 0; i < solveme.length; i++) {
    /* make sure input boxes don't auto-anything */
    solveme[i].setAttribute("autocomplete","off");
    solveme[i].setAttribute("autocorrect", "off");
    solveme[i].setAttribute("autocapitalize", "off");
    solveme[i].setAttribute("spellcheck", "false");
    solveme[i].value = "";

    /* adjust answer for ignorecase or nospaces */
    var cl = solveme[i].classList;
    var real_answer = solveme[i].dataset.answer;
    if (cl.contains("ignorecase")) {
      real_answer = real_answer.toLowerCase();
    }
    if (cl.contains("nospaces")) {
      real_answer = real_answer.replace(/ /g, "");
    }
    solveme[i].dataset.answer = real_answer;

    /* attach checking function */
    solveme[i].onkeyup = solveme_func;
    solveme[i].onchange = solveme_func;

    solveme[i].insertAdjacentHTML("afterend", " <span class='webex-icon'></span>")
  }

  /* set up radiogroups */
  var radiogroups = document.getElementsByClassName("webex-radiogroup");
  for (var i = 0; i < radiogroups.length; i++) {
    radiogroups[i].onchange = radiogroups_func;
  }

  /* set up selects */
  var selects = document.getElementsByClassName("webex-select");
  for (var i = 0; i < selects.length; i++) {
    selects[i].onchange = select_func;
    selects[i].insertAdjacentHTML("afterend", " <span class='webex-icon'></span>")
  }

  update_total_correct();
}

</script><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./12_knn.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">K-Nearest Neighbors</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./14_dimension.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Dimensionality Reduction</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">STA 3311</div>
  </div>
</footer>


</body></html>