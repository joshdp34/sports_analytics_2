[
  {
    "objectID": "11_Spatial.html#player-tracking-data-overview",
    "href": "11_Spatial.html#player-tracking-data-overview",
    "title": "\n11  Visualization of Player Tracking Data\n",
    "section": "\n11.1 Player Tracking Data Overview",
    "text": "11.1 Player Tracking Data Overview\nPlayer tracking data typically consists of time-stamped x- and y-coordinates representing the player’s position on the field or court. This data can capture high-frequency movements and is usually collected through optical tracking systems or wearable devices.\nThe example below contains the shots taken by Dirk Nowitzki during a game against the Portland Trailblazers on March 20, 2016.\n\nlibrary(tidyverse)\n\ndat = read_csv(\"data/shots_nowitzky.csv\")\n\ndat |&gt; glimpse()\n\nRows: 26\nColumns: 24\n$ GRID_TYPE           &lt;chr&gt; \"Shot Chart Detail\", \"Shot Chart Detail\", \"Shot Ch…\n$ GAME_ID             &lt;dbl&gt; 21501037, 21501037, 21501037, 21501037, 21501037, …\n$ GAME_EVENT_ID       &lt;dbl&gt; 52, 64, 68, 151, 169, 180, 207, 214, 283, 286, 311…\n$ PLAYER_ID           &lt;dbl&gt; 1717, 1717, 1717, 1717, 1717, 1717, 1717, 1717, 17…\n$ PLAYER_NAME         &lt;chr&gt; \"Dirk Nowitzki\", \"Dirk Nowitzki\", \"Dirk Nowitzki\",…\n$ TEAM_ID             &lt;dbl&gt; 1610612742, 1610612742, 1610612742, 1610612742, 16…\n$ TEAM_NAME           &lt;chr&gt; \"Dallas Mavericks\", \"Dallas Mavericks\", \"Dallas Ma…\n$ PERIOD              &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,…\n$ MINUTES_REMAINING   &lt;dbl&gt; 7, 6, 6, 1, 11, 10, 8, 7, 2, 1, 0, 9, 9, 3, 2, 2, …\n$ SECONDS_REMAINING   &lt;dbl&gt; 37, 43, 26, 21, 48, 36, 33, 59, 21, 51, 8, 49, 20,…\n$ EVENT_TYPE          &lt;chr&gt; \"Made Shot\", \"Missed Shot\", \"Missed Shot\", \"Missed…\n$ ACTION_TYPE         &lt;chr&gt; \"Turnaround Fadeaway shot\", \"Jump Shot\", \"Jump Sho…\n$ SHOT_TYPE           &lt;chr&gt; \"2PT Field Goal\", \"2PT Field Goal\", \"3PT Field Goa…\n$ SHOT_ZONE_BASIC     &lt;chr&gt; \"Mid-Range\", \"Mid-Range\", \"Above the Break 3\", \"In…\n$ SHOT_ZONE_AREA      &lt;chr&gt; \"Left Side(L)\", \"Center(C)\", \"Center(C)\", \"Left Si…\n$ SHOT_ZONE_RANGE     &lt;chr&gt; \"8-16 ft.\", \"16-24 ft.\", \"24+ ft.\", \"8-16 ft.\", \"L…\n$ SHOT_DISTANCE       &lt;dbl&gt; 11, 18, 24, 9, 1, 14, 15, 16, 16, 12, 16, 25, 18, …\n$ LOC_X               &lt;dbl&gt; -106, -35, -45, -73, 2, 137, -20, -142, 161, -124,…\n$ LOC_Y               &lt;dbl&gt; 33, 183, 242, 57, 16, 46, 151, 75, 36, 13, 41, 214…\n$ SHOT_ATTEMPTED_FLAG &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ SHOT_MADE_FLAG      &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,…\n$ GAME_DATE           &lt;dbl&gt; 20160320, 20160320, 20160320, 20160320, 20160320, …\n$ HTM                 &lt;chr&gt; \"DAL\", \"DAL\", \"DAL\", \"DAL\", \"DAL\", \"DAL\", \"DAL\", \"…\n$ VTM                 &lt;chr&gt; \"POR\", \"POR\", \"POR\", \"POR\", \"POR\", \"POR\", \"POR\", \"…\n\n\nThe dataset contains the positions of Dirk’s shots along with some other variables. Note the two variables LOC_X and LOC_Y. These will be used as the location of the shot.\nLet’s start with just a simple scatterplot.\n\ndat |&gt; ggplot(aes(x = LOC_X, y = LOC_Y, col = factor(SHOT_MADE_FLAG)))+\n  geom_point()\n\n\n\n\nThe scatterplot shows the locations of the shots but we cannot determine where on the court these shots are taking place without knowing what the x and y coordinates are referring to in relation to the court. A display of a basketball court will make any insights on the shots easier."
  },
  {
    "objectID": "11_Spatial.html#plot-playing-area",
    "href": "11_Spatial.html#plot-playing-area",
    "title": "\n11  Visualization of Player Tracking Data\n",
    "section": "\n11.2 Plot Playing Area",
    "text": "11.2 Plot Playing Area\nIn R, plotting the playing area can be accomplished through sport-specific packages or custom-built functions using ggplot2.\n\n11.2.1 Basketball Court\nFor basketball courts, the sportyR package provides quick and accurate court visualizations.\n\nlibrary(sportyR)\n\ngeom_basketball(\"nba\") +\n  theme_minimal()\n\n\n\n\nWe have options to change the colors if we wish as well as only showing half the court.\n\ncourt_plot = geom_basketball(\"nba\", display_range = \"offense\",\n                             color_updates = list(\n                               offensive_half_court = \"#B8C4CA\",\n                               defensive_half_court = \"#B8C4CA\",\n                               court_apron = \"#00538C\",\n                               two_point_range = c(\"#e8e0d7\", \"#ffffff66\"),\n                               center_circle_fill = \"#B8C4CA\",\n                               painted_area = c(\"#B8C4CA\", \"#ffffff00\"),\n                               free_throw_circle_fill = \"#e8e0d7\",\n                               sideline = \"#13294b\",\n                               endline = \"#13294b\",\n                               division_line = \"#13294b\",\n                               center_circle_outline = \"#13294b\",\n                               lane_boundary = c(\"#ffffff\", \"#ffffff00\"),\n                               three_point_line = c(\"#13294b\", \"#ffffff\"),\n                               free_throw_circle_outline = \"#ffffff\",\n                               lane_space_mark = \"#ffffff\",\n                               restricted_arc = \"#13294b\",\n                               backboard = \"#13294b\"\n                             )) +\n  theme_minimal()\n\ncourt_plot\n\n\n\n\nIf we were to plot Dirk’s shots with this court, we obtain:\n\ncourt_plot +\n  geom_point(data = dat, aes(x = LOC_X, y = LOC_Y, \n                             col = factor(SHOT_MADE_FLAG)),\n             size = 3)\n\n\n\n\nWe see these shots are not matching up. The coordinates of the court start at (0,0) in the center and the court is horizontal. The shots by Dirk in dat have the coordinates with (0,0) at the center of the basket with the court vertical. We can adjust the court with some options in geom_basketball.\n\ncourt_plot = geom_basketball(\"nba\", display_range = \"defense\",\n                             rotation=90,\n                             x_trans = 41.75,\n                             color_updates = list(\n                               offensive_half_court = \"#B8C4CA\",\n                               defensive_half_court = \"#B8C4CA\",\n                               court_apron = \"#00538C\",\n                               two_point_range = c(\"#e8e0d7\", \"#ffffff66\"),\n                               center_circle_fill = \"#B8C4CA\",\n                               painted_area = c(\"#B8C4CA\", \"#ffffff00\"),\n                               free_throw_circle_fill = \"#e8e0d7\",\n                               sideline = \"#13294b\",\n                               endline = \"#13294b\",\n                               division_line = \"#13294b\",\n                               center_circle_outline = \"#13294b\",\n                               lane_boundary = c(\"#ffffff\", \"#ffffff00\"),\n                               three_point_line = c(\"#13294b\", \"#ffffff\"),\n                               free_throw_circle_outline = \"#ffffff\",\n                               lane_space_mark = \"#ffffff\",\n                               restricted_arc = \"#13294b\",\n                               backboard = \"#13294b\"\n                             )) +\n  theme_minimal()\n\ncourt_plot\n\n\n\n\nOur last adjustment deals with the different units of measurement between the court coordinates and the shot data coordinates. The court coordinates are in feet. The coordinates of the shots are in feet times 10. We will make the adjustment and then plot the shots on the court.\n\ncourt_plot +\n  geom_point(data = dat, aes(x = LOC_X/10, y = LOC_Y/10, \n                             col = factor(SHOT_MADE_FLAG)),\n             size = 3)\n\n\n\n\n\n11.2.2 Other playing surfaces\nsportyR also supports other playing surfaces.\nFootball Field\n\ngeom_football(\"nfl\") +\n  theme_minimal()\n\n\n\n\nBaseball Diamonds\n\ngeom_baseball(\"mlb\") +\n  theme_minimal()\n\n\n\n\nHockey Rink\n\ngeom_hockey(\"nhl\") +\n  theme_minimal()\n\n\n\n\nSoccer Field\n\ngeom_soccer(\"fifa\") +\n  theme_minimal()"
  },
  {
    "objectID": "11_Spatial.html#nfl-data",
    "href": "11_Spatial.html#nfl-data",
    "title": "\n11  Visualization of Player Tracking Data\n",
    "section": "\n11.3 NFL Data",
    "text": "11.3 NFL Data\nIn the nfl_player_track.csv data, the locations of players is provided for different time points in a play. This particular game is for the Dallas Cowboys vs the New Orleans Saints in week 13 of the 2018 season.\n\ndat2 = read_csv(\"data/nfl_player_track.csv\")\n\nThe data have the coordinate (0,0) at the left bottom of the fields when plotted horizontally. The coordinate (0,0) in geom_football is at the middle of the field. We can make the adjustment to have the coordinates match.\n\nfield = geom_football(\"nfl\",\n                      x_trans = 60,\n                      y_trans = 26.67) +\n  theme_minimal()\n\nfield\n\n\n\n\nWe will examine one play. The position of each player moves during the play. The different movements are defined by the frame number (frameId).\n\ndat_frame = dat2 |&gt; \n  filter(playId == 74) |&gt; \n  filter(frameId==1)\n\nfield+\n  geom_point(data = dat_frame, aes(x = x, y=y, col = team))\n\n\n\n\nNote that the data only includes the skill players. The offensive and defensive linemen are not included in the data."
  },
  {
    "objectID": "11_Spatial.html#animating-movement-with-gganimate",
    "href": "11_Spatial.html#animating-movement-with-gganimate",
    "title": "\n11  Visualization of Player Tracking Data\n",
    "section": "\n11.4 Animating Movement with gganimate",
    "text": "11.4 Animating Movement with gganimate\nStatic plots give valuable but limited insights. Animating player tracking data offers richer context by capturing player dynamics over time. gganimate elegantly extends ggplot2 to add temporal dimensions to our visualizations.\nThe football dataset includes the frame_Id. We can animate a play by animating over the frame_Id.\n\nlibrary(gganimate)\n\n\ndat_play = dat2 |&gt; \n  filter(playId == 74) \n\n\np = field +\n  geom_point(data=dat_play, \n             aes(x = x, y = y, \n                 group = nflId, \n                 color = factor(team)),\n             size = 5) +\n  scale_color_manual(values=c(\"#D3BC8D\", \"#815337\", \"#003594\"))+\n  theme(legend.position = \"none\")+\n  transition_manual(frameId) +\n  ease_aes('linear')\n\nanimate(p, fps = 10, width = 1600, height = 900)\n\n#to save the animation\nanim_save(\"play01.gif\")"
  },
  {
    "objectID": "12_knn.html#soccer-player-value",
    "href": "12_knn.html#soccer-player-value",
    "title": "\n12  K-Nearest Neighbors\n",
    "section": "\n12.1 Soccer Player Value",
    "text": "12.1 Soccer Player Value\nWe will examine value of soccer players in in the video game FIFA 2021. We want to model the value of the player based on some descriptive variables (age, height, weight) and game ratings.\nThere are a number of variables that have a value of 0. We will exclude them. In addition, we will exclude goal keepers.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndat = read_csv(\"data/soccer_players.csv\")\n\ndat = dat |&gt; \n  #remove goal_keepers\n  filter(!grepl(pattern = \"GK\", x = player_positions)) |&gt; \n  select(short_name, age, height_cm, club_name,\n         weight_kg,team_position, pace:physic, value_eur)\n\ndat = dat |&gt; \n  filter(value_eur&gt;0)\n\nWe will start by just focusing on the club AC Monza.\n\ndat2 = dat |&gt; filter(club_name==\"AC Monza\")"
  },
  {
    "objectID": "12_knn.html#predictions-based-on-neighbors",
    "href": "12_knn.html#predictions-based-on-neighbors",
    "title": "\n12  K-Nearest Neighbors\n",
    "section": "\n12.2 Predictions Based on Neighbors",
    "text": "12.2 Predictions Based on Neighbors\nLet’ look at the two variables defending and shooting only. Below are these rating plotted with the value in euros displayed with a continuous color gradient.\n\ndat2 |&gt; ggplot(aes(x = shooting, y = defending, col = value_eur))+\n  geom_point(size = 3)+\n  scale_color_gradient(low=\"blue\", high=\"red\")\n\n\n\n\nSuppose we have a new player who had a shooting rating of 60 and a defending rating of 30. This new player is shown in the plot below as a black point.\n\n\n\n\n\nWhat would be the best prediction of this new point? Instead of using a model and a probability distribution to determine this prediction, we can look at the neighboring points.\nIn the plot below, the three closest points are connected to the new point with black lines.\n\n\n\n\n\nWe could then average the value for these three nearest neighbors and use that as the prediction of the new point. This would lead to a prediction of \\[\n\\frac{625000+900000+750000}{3}=758333.3\n\\]\nWhat if we choose to pick the five closest neighbors and average those for the prediction?\n\n\n\n\n\nThis would lead to a prediction of \\[\n\\frac{625000+900000+750000+500000+2400000}{5}=1035000\n\\]\nWe see that changing how many neighbors we use for the average plays a role in what the prediction will be."
  },
  {
    "objectID": "12_knn.html#k-nearest-neighbors-in-tidymodels",
    "href": "12_knn.html#k-nearest-neighbors-in-tidymodels",
    "title": "\n12  K-Nearest Neighbors\n",
    "section": "\n12.3 K-Nearest Neighbors in Tidymodels",
    "text": "12.3 K-Nearest Neighbors in Tidymodels\nInstead of looking at just one club, let’s look at all of them. Below is a scatterplot for all players. Due to the skewness of the value_eur variable, we took the log first.\n\ndat |&gt; ggplot(aes(x = shooting, y = defending, col = log(value_eur)))+\n  geom_point(size = 3)+\n  scale_color_gradient(low=\"blue\", high=\"red\")\n\n\n\n\n\n12.3.1 Train-Test split\nAs we have done in the past, we split the data into testing and training sets.\n\nset.seed(1004)\n\ndat_split = initial_split(dat)\ntrain = training(dat_split)\ntest = testing(dat_split)\n\nNext, we create a pre-processing recipe. In k-NN, it is vital to standardize numeric features, as the algorithm is distance-based, and scales of features can significantly influence results. For example, if we had age as a feature along with height, the units of measurement for height may have a larger influence on which points are the closest neighbors than with age.\nIf we have any categorical features, we would need to convert them into dummy variables.\n\nsoc_rec = recipe(data = train,\n                 value_eur ~ shooting + defending) |&gt; \n  step_log(value_eur) |&gt; \n  step_normalize(all_predictors()) |&gt; \n  prep()\n\n\ntrain_dat = soc_rec |&gt; bake(new_data = NULL)\ntest_dat = soc_rec |&gt; bake(test)\n\n\n12.3.2 Tuning the number of neighbors\nSelecting an optimal (k) is essential, as it strongly influences model performance. Tidymodels supports tuning via cross-validation as we have done in previous methods.\n\nmodel = nearest_neighbor(neighbors = tune()) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\")\n\nThe range for neighbors can be tricky. Best to start low and then increase the max value if needed. Below, we will go to a max of 60 neighbors.\n\ntune_grid = grid_regular(\n  neighbors(range=c(2, 60)),\n  levels = 20\n)\n\ndat_folds = vfold_cv(train_dat, v = 5)\n\nTuning the model over the grid may take some time. In this scenario, it should take just a few seconds.\n\ntune_results = tune_grid(\n  model,\n  value_eur ~ shooting + defending,\n  resamples = dat_folds,\n  grid = tune_grid\n)\n\nLet’s plot the RMSE and \\(R^2\\) values for the different value of neighbors.\n\ntune_results |&gt; autoplot()\n\n\n\n\nSometimes, there will not be a minimum RMSE value or a maximum \\(R^2\\). In the plot above, both metrics continue to improve as the number of neighbors increases. We could go back and increase the max in the range for neighbors in the grid, or we could examine these plots and notice that the metrics are continuing to improve but by a small amount. In other words, they are leveling out.\nLet’s first make sure the best metric is at the max.\n\nbest_param = tune_results |&gt;  select_best(metric = \"rsq\")\n\nbest_param\n\n# A tibble: 1 × 2\n  neighbors .config              \n      &lt;int&gt; &lt;chr&gt;                \n1        60 Preprocessor1_Model20\n\n\nWe see the max value of neighbors is the best by rsq. If you changed to metric to rmse it would also give the max.\nExamining the plot above, it appears at 30 neighbors, both metrics have for the most part leveled off. Let’s use 30 for the number of neighbors.\n\nmodel = nearest_neighbor(neighbors = 30) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\")\n\n\nfit = model |&gt; fit(value_eur ~ shooting + defending, data=train_dat)\n\nWith the training data, let’s examine the \\(R^2\\) value of the fit.\n\npreds = fit |&gt; predict(new_data = train_dat) |&gt; \n  cbind(train_dat)\n\nrsq(data = preds,\n    truth = value_eur,\n    estimate=.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.781\n\n\nTo see how well this fit will predict new data, let’s see the \\(R^2\\) value for the test data.\n\npreds = fit |&gt; predict(new_data = test_dat) |&gt; \n  cbind(test_dat)\n\nrsq(data = preds,\n    truth = value_eur,\n    estimate=.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.748\n\n\nWe see the value fo \\(R^2\\) is about 0.034 lower for the test than for the train. Thus, we have slight overfitting but it is not severe.\n\n12.3.3 Using all the Features\nLet’s now fit a k-NN model with all the features available.\n\nsoc_rec = recipe(data = train,\n                 value_eur ~ .) |&gt; \n  step_rm(short_name, club_name) |&gt; \n  step_log(value_eur) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_normalize(all_predictors()) |&gt; \n  prep()\n\n\ntrain_dat = soc_rec |&gt; bake(new_data = NULL)\ntest_dat = soc_rec |&gt; bake(test)\n\nmodel = nearest_neighbor(neighbors = tune()) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\")\n\ntune_grid = grid_regular(\n  neighbors(range=c(2, 60)),\n  levels = 20\n)\n\ndat_folds = vfold_cv(train_dat, v = 5)\n\ntune_results = tune_grid(\n  model,\n  value_eur ~ .,\n  resamples = dat_folds,\n  grid = tune_grid\n)\n\ntune_results |&gt; autoplot()\n\n\n\nbest_param = tune_results |&gt;  select_best(metric = \"rsq\")\n\nbest_param\n\n# A tibble: 1 × 2\n  neighbors .config              \n      &lt;int&gt; &lt;chr&gt;                \n1        17 Preprocessor1_Model06\n\nmodel = nearest_neighbor(neighbors = best_param$neighbors) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\")\n\n\nfit = model |&gt; fit(value_eur ~ ., data=train_dat)\n\npreds = fit |&gt; predict(new_data = train_dat) |&gt; \n  cbind(train_dat)\n\nrsq(data = preds,\n    truth = value_eur,\n    estimate=.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.914\n\npreds = fit |&gt; predict(new_data = test_dat) |&gt; \n  cbind(test_dat)\n\nrsq(data = preds,\n    truth = value_eur,\n    estimate=.pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.859"
  },
  {
    "objectID": "13_clustering.html#nba-player-styles",
    "href": "13_clustering.html#nba-player-styles",
    "title": "\n13  Clustering\n",
    "section": "\n13.1 NBA Player Styles",
    "text": "13.1 NBA Player Styles\nTo demonstrate the power of clustering in a sports context, we begin with a case study using player statistics from the NBA. Our goal is to group players based on their in-game performance during a single season, allowing us to identify archetypes such as volume scorers, defensive anchors, or well-rounded contributors. By clustering players based on key statistical measures, we can uncover patterns that may not be obvious from traditional per-game summaries.\nIn this case study, we use the hoopR package in R, which provides access to rich and detailed NBA player box score data via the ESPN API. Specifically, we’ll use data from the 2021–2022 season, focusing on player-level averages across several core performance metrics.\nWe begin by loading the necessary libraries and retrieving the data. The tidymodels ecosystem helps us structure our data processing and modeling workflow, while hoopR provides the raw game-level data.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(hoopR)\n\nThe load_nba_player_box() function fetches player box score data for the season. We then clean and summarize the data to compute per-game averages for each player. These averages form the basis of our clustering analysis.\n\nlibrary(janitor)\n\n# Get NBA player box scores for the 2021–2022 season\nnba_data &lt;- load_nba_player_box(season = 2022)\n\nThe raw box score data includes one row per player per game. Since we are interested in clustering based on overall performance style rather than single-game variation, we aggregate the data to the player level. We compute average points (PTS), assists (AST), rebounds (REB), steals (STL), blocks (BLK), turnovers (TOV), three-pointers made (FG3M), field goal attempts (FGA), and free throw attempts (FTA), and restrict our analysis to players who appeared in at least 30 games during the season to ensure we are analyzing a stable sample.\n\nplayer_stats &lt;- nba_data |&gt;\n  group_by(athlete_display_name, team_abbreviation) |&gt;\n  summarise(\n    games = n(),\n    pts = mean(points, na.rm = TRUE),\n    ast = mean(assists, na.rm = TRUE),\n    reb = mean(rebounds, na.rm = TRUE),\n    stl = mean(steals, na.rm = TRUE),\n    blk = mean(blocks, na.rm = TRUE),\n    tov = mean(turnovers, na.rm = TRUE),\n    fg3m = mean(three_point_field_goals_made, na.rm = TRUE),\n    ftm = mean(free_throws_made, na.rm = TRUE),\n    position = collapse::fmode(athlete_position_abbreviation),\n    .groups = \"drop\"\n  ) |&gt;\n  filter(games &gt;= 30)\n\nBefore performing clustering, we standardize the numeric features. This is a crucial step because the variables we are using are on different scales. For instance, points per game typically ranges from single digits to the low 30s, while steals or blocks per game rarely exceed 2 or 3. Without scaling, clustering algorithms like K-means will be unduly influenced by the variables with larger numeric ranges.\n\n# Scale the numeric columns\nscaled_stats &lt;- recipe(~., data = player_stats) |&gt; \n  step_rm(athlete_display_name, team_abbreviation, position) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  prep()\n\ndat = scaled_stats |&gt; bake(new_data=NULL)\n\nThe goal of this clustering exercise is not just to segment players for the sake of grouping, but to discover meaningful player types. For example, we may find a cluster of players who attempt many field goals and score efficiently, representing primary scorers. Another group may consist of players with high assist and rebound numbers, pointing to versatile point-forwards or playmakers. Defensive specialists may emerge as a group with high block and steal rates but low scoring output."
  },
  {
    "objectID": "13_clustering.html#k-means-clustering",
    "href": "13_clustering.html#k-means-clustering",
    "title": "\n13  Clustering\n",
    "section": "\n13.2 K-means Clustering",
    "text": "13.2 K-means Clustering\nNow that we have prepared our NBA player statistics data, the next step is to apply K-means clustering, one of the most widely used and intuitive clustering algorithms. K-means aims to partition the observations into a fixed number of groups, or clusters, such that each observation belongs to the cluster with the nearest centroid—the mean position of all points within that cluster.\n\n13.2.1 How K-means Works\nK-means clustering is an iterative algorithm that seeks to minimize the total within-cluster sum of squares (WCSS), which measures the variance of the observations around the cluster centroids. Here’s how the algorithm works conceptually:\n\nChoose the number of clusters, \\(k\\), to create.\nRandomly assign initial positions for the \\(k\\) centroids.\nAssign each observation to the cluster whose centroid is closest, based on Euclidean distance.\nRecompute the centroids as the mean of the observations in each cluster.\nRepeat steps 3 and 4 until the assignments stop changing or a maximum number of iterations is reached.\n\nThe algorithm always converges, but it may converge to a local minimum depending on the starting positions. Therefore, it is common practice to run the algorithm multiple times with different random starts and retain the best result.\n\n13.2.2 Example with two features\nSuppose we only looked at a sample of 30 players with the features reb and ast.\n\n\n\n\n\nWe can see what appears to be three grouping of players:\n\n\n\n\n\nHow do we get an unsupervised method to determine these clusters instead of us visually determining them?\nWe can start by specifying that there will be three cluster and putting three points at random on the plot. We will call these points the centroids. They are the orange points in the plot below.\n\n\n\n\n\nWe now find the distance from each ppoint to the closest centroid.\n\n\n\n\n\nWe now cluster those points that share a centroid (same color in the plot below) and then update the centroids by calculating the middle of the points in the cluster.\n\n\n\n\n\nWe now repeat the process. That is, we determine which centroid is closest to each point.\n\n\n\n\n\nNext, color code the points based on the closest centroid and recalculate the position of the centroid.\n\n\n\n\n\nThis process continues until the centroids no longer moves.\nWith more than two features, the idea is still the same: calculate the distance to each centroid and make clusters based on those that are closest. Then we update the centroids and repeat the process. The difference with higher number of features is that we can no longer visualize the process as we have just done here."
  },
  {
    "objectID": "13_clustering.html#choosing-the-number-of-clusters",
    "href": "13_clustering.html#choosing-the-number-of-clusters",
    "title": "\n13  Clustering\n",
    "section": "\n13.3 Choosing the Number of Clusters",
    "text": "13.3 Choosing the Number of Clusters\nOne of the most important decisions in K-means clustering is selecting an appropriate value for \\(k\\), the number of clusters. Too few clusters can lead to overly broad groupings that obscure important differences, while too many clusters may produce noise or overfit to small nuances in the data.\nA commonly used heuristic is the Elbow Method, which plots the total WCSS for different values of \\(k\\). The idea is to choose the number of clusters at which the rate of decrease in WCSS sharply slows down—resembling an “elbow” in the plot. This point represents a good trade-off between reducing within-cluster variance and avoiding overly complex models.\nA handy plot for doing this is the fviz_nbclust plot in the factoextra package. Below, we examine the plot for all the players but with just the features reb and ast.\n\nlibrary(factoextra)\nfviz_nbclust(dat |&gt; select(reb, ast) , kmeans, method = \"wss\")\n\n\n\n\nHere we see the decrees in WCSS starts to level off at \\(k=5\\). This suggests that a five-cluster solution captures most of the structure in the data without unnecessary complexity."
  },
  {
    "objectID": "13_clustering.html#applying-k-means-to-nba-player-data",
    "href": "13_clustering.html#applying-k-means-to-nba-player-data",
    "title": "\n13  Clustering\n",
    "section": "\n13.4 Applying K-means to NBA Player Data",
    "text": "13.4 Applying K-means to NBA Player Data\nLet’s first determine the number of clusters we should use when using all 9 features.\n\nfviz_nbclust(dat, kmeans, method = \"wss\", k.max = 20)\n\n\n\n\nIn this situation, we see the number of knots is not easily chosen by the elbow method. The decrease in WCSS is consistent, for the most part, throughout the values of k.\nSince there are traditionally five positions on a basketball team, let’s go with \\(k=5\\).\nOnce we have selected the number of clusters, we fit the K-means model using the kmeans() function in base R. We use the nstart = 25 argument to perform the clustering 25 times with different initial centroids and keep the best solution based on the total WCSS.\n\nkmeans_fit &lt;- kmeans(dat, centers = 5, nstart = 25)\n\nThe kmeans_fit object contains several components, including the cluster assignments for each observation, the coordinates of the centroids, and the total within-cluster sum of squares. We add the cluster assignments to our original player-level dataset for further analysis.\n\nplayer_stats &lt;- player_stats |&gt;\n  mutate(cluster = factor(kmeans_fit$cluster))"
  },
  {
    "objectID": "13_clustering.html#interpreting-the-clusters",
    "href": "13_clustering.html#interpreting-the-clusters",
    "title": "\n13  Clustering\n",
    "section": "\n13.5 Interpreting the Clusters",
    "text": "13.5 Interpreting the Clusters\nThe most meaningful part of a clustering analysis is the interpretation of the resulting groups. To do this, we examine the average values of the original variables within each cluster. This helps us understand what defines each group.\n\nplayer_stats |&gt;\n  group_by(cluster) |&gt;\n  summarise(across(pts:ftm, mean, na.rm = TRUE))\n\n# A tibble: 5 × 9\n  cluster   pts   ast   reb   stl   blk   tov  fg3m   ftm\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1       13.1  2.78   3.82 0.893 0.361 1.40  1.91  1.81 \n2 2       22.2  6.28   6.39 1.21  0.516 3.03  2.10  4.32 \n3 3        4.36 0.930  2.41 0.350 0.247 0.568 0.544 0.573\n4 4       11.9  1.87   8.03 0.758 1.20  1.42  0.537 1.96 \n5 5        6.05 1.27   2.95 0.535 0.310 0.704 0.705 0.836\n\n\nThis summary can be used to determine similar players in each cluster. Let’s look at the distribution of positions for the second cluster which has the highest aveage points and assists.\n\nplayer_stats |&gt; filter(cluster == 2) |&gt; \n  select(position) |&gt; \n  table()\n\nposition\n C  F PF PG SF SG \n 3  1  5 19  7 13 \n\n\nMost of these players in cluster 2 are guards. Let’s look at the players in this cluster.\n\nplayer_stats |&gt; \n  filter(cluster==2) |&gt; \n  select(athlete_display_name)\n\n# A tibble: 48 × 1\n   athlete_display_name\n   &lt;chr&gt;               \n 1 Anthony Edwards     \n 2 Bradley Beal        \n 3 Brandon Ingram      \n 4 CJ McCollum         \n 5 Cade Cunningham     \n 6 Chris Paul          \n 7 Cole Anthony        \n 8 D'Angelo Russell    \n 9 Darius Garland      \n10 De'Aaron Fox        \n# ℹ 38 more rows"
  },
  {
    "objectID": "13_clustering.html#visualizing-the-results",
    "href": "13_clustering.html#visualizing-the-results",
    "title": "\n13  Clustering\n",
    "section": "\n13.6 Visualizing the Results",
    "text": "13.6 Visualizing the Results\nTo help interpret the clusters visually, we can use the fviz_cluster() function, which projects the high-dimensional data onto two principal components and shows the cluster memberships in two dimensions.\n\nfviz_cluster(list(data = dat, cluster = kmeans_fit$cluster))\n\n\n\n\nWhile this visualization simplifies the data into two dimensions, it still provides a helpful overview of how well the clusters are separated and whether there is any substantial overlap between groups. Large, well-separated clusters suggest that the statistical profiles of the players are meaningfully different."
  },
  {
    "objectID": "13_clustering.html#hierarchical-clustering",
    "href": "13_clustering.html#hierarchical-clustering",
    "title": "\n13  Clustering\n",
    "section": "\n13.7 Hierarchical Clustering",
    "text": "13.7 Hierarchical Clustering\nWhile K-means clustering is powerful and widely used, it requires the analyst to choose the number of clusters in advance and is most effective when clusters are roughly spherical and of similar size. An alternative technique that overcomes some of these limitations is hierarchical clustering, which builds a tree-like structure of nested clusters. This method provides a flexible and informative view of the relationships among observations and is particularly helpful when we want to explore the data at multiple levels of granularity.\n\n13.7.1 The Basic Idea\nHierarchical clustering operates by computing a measure of dissimilarity (or distance) between each pair of observations, then successively merging (or, in some cases, splitting) observations and clusters based on that measure. The result is a dendrogram—a tree diagram that illustrates how observations group together.\nThere are two primary types of hierarchical clustering:\n\n\nAgglomerative clustering, which begins with each observation in its own cluster and repeatedly merges the two closest clusters.\n\nDivisive clustering, which starts with all observations in a single cluster and successively splits them.\n\nIn practice, agglomerative clustering is far more common and is the default approach in most statistical software.\n\n13.7.2 Distance Metrics and Linkage Methods\nHierarchical clustering requires two key decisions: how to measure the distance between observations, and how to define the distance between clusters.\nThe most common distance metric is Euclidean distance, which measures the straight-line distance between two points in multidimensional space. Other options include Manhattan distance and cosine similarity, though Euclidean is usually sufficient for standardized numerical data.\nOnce distances between individual observations are calculated, we must decide how to compute the distance between clusters. This is known as the linkage method, and several options are available:\n\n\nSingle linkage: the shortest distance between any two points in the two clusters.\n\nComplete linkage: the greatest distance between any two points.\n\nAverage linkage: the average of all pairwise distances.\n\nWard’s method: minimizes the total within-cluster variance and tends to produce compact, spherical clusters (similar in spirit to K-means).\n\nFor our analysis of NBA player statistics, we will use Ward’s method, which is generally well-suited for quantitative data and tends to create clusters of similar size.\n\n13.7.3 Applying Hierarchical Clustering to NBA Player Data\nWe begin by computing the distance matrix and fitting the hierarchical clustering model.\n\n# Remove player names and compute Euclidean distances\ndist_matrix &lt;- dist(dat )\n\n# Perform hierarchical clustering using Ward's method\nhc &lt;- hclust(dist_matrix, method = \"ward.D2\")\n\nThe hclust() function returns an object containing the hierarchy of merges, which we can visualize using a dendrogram.\n\nfviz_dend(hc, k = 5, rect = TRUE, labels_track_height = 3)\n\n\n\n\nIn the dendrogram, each leaf node represents an individual player. As we move up the tree, branches merge into larger clusters based on their similarity. The height at which two clusters are merged corresponds to the dissimilarity between them.\nWe can use this tree to cut the data into any number of clusters by drawing a horizontal line across the dendrogram. For example, if we cut the tree at \\(k = 5\\), we obtain four clusters similar to our K-means example.\n\ncluster_assignments &lt;- cutree(hc, k = 5)\n\n# Add cluster labels to the player data\nplayer_stats$hc_cluster &lt;- factor(cluster_assignments)\n\n\n13.7.4 Interpreting and Comparing Results\nOnce we have assigned players to clusters using hierarchical clustering, we can interpret the groups in the same way as we did with K-means. For example, we might summarize the mean statistics for each cluster:\n\nplayer_stats |&gt;\n  group_by(hc_cluster) |&gt;\n  summarise(across(pts:ftm, mean, na.rm = TRUE))\n\n# A tibble: 5 × 9\n  hc_cluster   pts   ast   reb   stl   blk   tov  fg3m   ftm\n  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1           7.06  1.46  3.14 0.583 0.336 0.792 0.878 0.957\n2 2           4.96  1.03  2.63 0.388 0.278 0.619 0.572 0.671\n3 3          13.1   2.10  8.55 0.804 1.21  1.58  0.644 2.23 \n4 4          14.5   2.84  3.99 0.876 0.341 1.47  2.12  2.06 \n5 5          20.3   6.10  5.81 1.26  0.518 2.86  2.04  3.82 \n\n\nThis output allows us to describe the average player profile for each cluster. It’s important to note that while hierarchical clustering and K-means may yield similar types of player groupings, they do not always agree. For example, hierarchical clustering may detect smaller subgroups or more gradual transitions between player types, since it is not constrained to fixed-size or spherical clusters.\nWe can also compare the cluster assignments from K-means and hierarchical clustering directly:\n\ntable(KMeans = player_stats$cluster, Hierarchical = player_stats$hc_cluster)\n\n      Hierarchical\nKMeans   1   2   3   4   5\n     1  31   4   1  65  10\n     2   0   0   4   1  43\n     3   4 120   0   0   0\n     4   6   5  37   0   1\n     5 127  12   0   0   0\n\n\nThis contingency table shows how much overlap exists between the two methods. A high degree of agreement suggests that the player groupings are robust and meaningful, while major differences may indicate that the choice of clustering method influences the interpretation.\n\n13.7.5 Advantages and Limitations\nOne key advantage of hierarchical clustering is that it provides a complete picture of how observations relate to one another at all levels of similarity. This is particularly useful in exploratory settings where the number of natural groupings is unclear. It also does not require the analyst to pre-specify the number of clusters, unlike K-means.\nHowever, hierarchical clustering has some limitations. It can be computationally expensive for large datasets, since it must compute and store the entire distance matrix. Additionally, once a merge is made, it cannot be undone, meaning that early decisions in the hierarchy can influence the final outcome even if better options become available later. As a result, hierarchical clustering is best suited for medium-sized datasets where interpretability is a primary concern."
  },
  {
    "objectID": "13_clustering.html#comparing-k-means-and-hierarchical-clustering",
    "href": "13_clustering.html#comparing-k-means-and-hierarchical-clustering",
    "title": "\n13  Clustering\n",
    "section": "\n13.8 Comparing K-means and Hierarchical Clustering",
    "text": "13.8 Comparing K-means and Hierarchical Clustering\nClustering is often an exploratory process, and choosing the “best” algorithm depends on both the structure of the data and the goals of the analysis. In this section, we compare the results of K-means and hierarchical clustering applied to our NBA player statistics dataset. Our goal is not only to assess which algorithm performs better, but also to explore how the insights generated by each method can differ in practical and strategic ways.\n\n13.8.1 Conceptual Differences\nBefore comparing the results quantitatively, it’s useful to revisit the conceptual differences between the two approaches.\nK-means is a partitioning method. It assumes that the data are divisible into a fixed number of clusters and tries to optimize the grouping by minimizing the total within-cluster variance. The clusters produced by K-means tend to be compact and evenly sized, which works well when the true groups are spherical and well-separated. However, the algorithm requires us to specify the number of clusters in advance and is sensitive to the initial placement of centroids.\nIn contrast, hierarchical clustering is a connectivity-based method. It does not require a predetermined number of clusters and instead builds a full hierarchy that shows relationships among observations at all levels of similarity. It can reveal nested or irregular groupings, and its results are typically presented as a dendrogram. This method offers more interpretive flexibility but can be computationally intensive and less effective when the dataset contains many noise points or overlapping clusters.\n\n13.8.2 Comparing Cluster Assignments\nTo see how the two methods compare on our NBA player dataset, we can tabulate the cluster assignments for each player. Recall that we created a cluster variable from K-means and a hc_cluster variable from hierarchical clustering.\n\ntable(KMeans = player_stats$cluster, Hierarchical = player_stats$hc_cluster)\n\n      Hierarchical\nKMeans   1   2   3   4   5\n     1  31   4   1  65  10\n     2   0   0   4   1  43\n     3   4 120   0   0   0\n     4   6   5  37   0   1\n     5 127  12   0   0   0\n\n\nThis cross-tabulation shows how many players were assigned to each pair of K-means and hierarchical clusters. Large values along the diagonal suggest strong agreement between the methods, while high off-diagonal values indicate differences. For example, if most players in K-means Cluster 1 are also in hierarchical Cluster 3, we might consider these to represent similar player types despite the differing algorithmic foundations.\nYou may find that some clusters align fairly well, particularly those representing extreme player types—such as high-scoring, high-usage stars or low-usage defensive specialists. On the other hand, more nuanced or hybrid players may be assigned to different clusters by each method depending on how the algorithm interprets the multidimensional space.\n\n13.8.3 Visual Comparison\nVisualizing the clusters produced by each method is another effective way to compare results. We can again use the fviz_cluster() function to project the data into two dimensions using principal component analysis and color-code the observations by their cluster membership.\n\nfviz_cluster(list(data = dat, cluster = kmeans_fit$cluster)) +\n  ggtitle(\"K-means Clustering of NBA Players\")\n\n\n\nfviz_dend(hc, k = 5, rect = TRUE, labels_track_height = 3) +\n  ggtitle(\"Hierarchical Clustering Dendrogram\")\n\n\n\n\nThese visualizations provide complementary views of the data. The K-means plot shows distinct, compact groups, while the dendrogram reveals the hierarchical structure—allowing us to see not only how players group together but also how those groups relate to one another.\n\n13.8.4 Statistical Comparison\nAlthough clustering is fundamentally an unsupervised technique, there are ways to assess and compare the quality of different clustering results. One popular metric is the Silhouette score, which measures how well each observation fits within its assigned cluster compared to other clusters.\nWe can compute the average silhouette width for both methods:\n\n# Silhouette score for K-means\nsil_kmeans &lt;- cluster::silhouette(kmeans_fit$cluster, dist_matrix)\nmean(sil_kmeans[, \"sil_width\"])\n\n[1] 0.2197183\n\n# Silhouette score for hierarchical clustering\nsil_hc &lt;- cluster::silhouette(cluster_assignments, dist_matrix)\nmean(sil_hc[, \"sil_width\"])\n\n[1] 0.1827008\n\n\nHigher average silhouette scores indicate better-defined clusters. While this metric should not be the sole basis for choosing a method, it can provide a helpful indication of how well the structure discovered by each algorithm matches the natural groupings in the data."
  },
  {
    "objectID": "14_dimension.html#introduction",
    "href": "14_dimension.html#introduction",
    "title": "\n14  Dimensionality Reduction\n",
    "section": "\n14.1 Introduction",
    "text": "14.1 Introduction\nIn the realm of statistics and data science, the ability to effectively analyze and draw insights from data is paramount. As we venture into the era of big data, we encounter datasets of increasing complexity and size. These datasets often comprise a vast number of variables, a situation described as high dimensionality. Understanding the concept of dimensionality reduction is essential, not just as an abstract mathematical idea but as a practical tool for making sense of complex data.\nDimensionality reduction sits at the heart of sports analytics, serving as a bridge between raw data and actionable insights. It addresses several critical challenges in data analysis, including the curse of dimensionality, noise in the dataset, and the difficulties involved in visualizing multidimensional data. By simplifying the data without significant loss of information, dimensionality reduction techniques enable us to build models that are not only more efficient but also more interpretable.\nThe study of dimensionality reduction offers a glimpse into the interdisciplinary nature of data science, where statistics, computer science, and domain expertise converge. This area highlights the importance of understanding both the theoretical foundations and the practical applications of statistical methods. While the mathematical underpinnings, such as linear algebra, are crucial, the focus here is on grasping the conceptual framework and the impact of dimensionality reduction on data analysis.\nThe introduction to dimensionality reduction begins with the rationale behind it. As datasets grow in size and complexity, the limitations of traditional analytical tools become apparent. The curse of dimensionality, a phenomenon where the data space expands so much that our data becomes sparse, affects not only the computational feasibility of models but also their performance. Reducing the number of input variables helps mitigate these issues, making models simpler, faster, and more generalizable.\nMoreover, in a practical setting, the reduction of dimensionality can be pivotal for noise reduction and visualization. By filtering out irrelevant or redundant features, we improve the model’s accuracy and reliability. Similarly, the transformation of high-dimensional data into a more manageable form allows for effective visualization, which is indispensable for data exploration and hypothesis generation.\nDimensionality reduction techniques, categorized into feature selection and feature extraction, offer a toolkit for addressing these challenges. Feature selection methods focus on identifying the most relevant features for the models. On the other hand, feature extraction techniques like Principal Component Analysis (PCA), transform the original features into a new set of variables that better capture the underlying structure of the data."
  },
  {
    "objectID": "14_dimension.html#reasons-for-dimensionality-reduction",
    "href": "14_dimension.html#reasons-for-dimensionality-reduction",
    "title": "\n14  Dimensionality Reduction\n",
    "section": "\n14.2 Reasons for Dimensionality Reduction",
    "text": "14.2 Reasons for Dimensionality Reduction\n\n14.2.1 Curse of Dimensionality:\nThe curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. It’s essential in understanding how dimensionality affects data analysis, leading to specific issues like data sparsity and increased computational complexity.\nUnderstanding the Curse of Dimensionality\nWhen the dimensionality increases, the volume of the space increases so rapidly that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Additionally, high-dimensional datasets are often accompanied by increased computational complexity and a greater chance of overfitting, making models less generalizable to new data.\nExample using the Batting dataset\nThe Batting dataset in the Lahman library consists of batting statistics for MLB players.\nLoading and Exploring the Data\n\nlibrary(Lahman)\nlibrary(tidyverse)\nlibrary(GGally)\ndata(\"Batting\")\n\nBelow is a scatterplot matrix of the pairs of 9 of the quantitative features for the Texas Rangers during the 2004 season.\n\nBatting |&gt; \n  filter(yearID==2004 & teamID==\"TEX\") |&gt; \n  select(R:BB) |&gt; \n  ggpairs()\n\n\n\n\nWith so many variables, it becomes cumbersome to visualize the relationships between the variables. The above scatterplot matrix shows the scatterplots of the 36 pairs of variables when you have nine features. It becomes difficult to examine all of these scatterplots. These plots also do not show any relationships between three variables at a time, and so forth.\nIn addition to visualization, the curse of dimenstionality also implies the sparseness of the data when we have more variables.\nLet’s first examine the variable R. Below is a dotplot for this variable.\n\nBatting |&gt; \n  filter(yearID==2004 & teamID==\"TEX\") |&gt; \n  ggplot(aes(x = R, y = 0))+\n  geom_point()+\n  ylab(\"\")+\n  theme(\n    axis.ticks.y=element_blank(),\n    axis.text.y=element_blank()\n    )\n\n\n\n\nNote that the min value of R is 0 and the max value is 114. There are 53 observations so we can think of the data for this variable as 53 pieces of information in that variable’s dimension (the \\(x\\) dimenstion on the plot above). Let’s calculate the ratio of information to the dimensional space:\n\\[\n\\frac{53}{114-0} = 0.4649\n\\]\nLet’s now examine another variable along with R. Below is the scatterplot of R and RBI\n\nBatting |&gt; \n  filter(yearID==2004 & teamID==\"TEX\") |&gt;\n  ggplot(aes(x = R, y = RBI))+\n  geom_point()\n\n\n\n\nFor RBI, the range of values are min = 0 and max = 112. The total dimensional space that these two variables take is \\[\n\\begin{align*}\n\\text{dimensional space for R}\\times\\text{dimensional space for RBI} &= 114 \\times 112\\\\\n&= 12768\n\\end{align*}\n\\]\nWe still only have 25 observations. The 53 pieces of information that we have in this two-dimensional space gives us the ratio \\[\n\\frac{53}{12768} = 0.0042\n\\]\nSo in two-dimensional space, the amount of data we have is a much lower ratio of the space than when we had in only one dimension.\nLet’s now add in a third variable BB. Note that the dimensional space for BB is min = 0 and max = 75. The size of the total dimensional space is \\[\n\\begin{align*}\n\\text{dim. space for R}\\times\\text{dim. space for RBI}\\times\\text{dim. space for BB} &= 114 \\times 112 \\times 75\\\\\n&= 957600\n\\end{align*}\n\\]\nAgain, we still only have 53 observations. So our 53 pieces of information only take up a ratio of \\[\n\\frac{53}{957600}=0.00006\n\\] of the 3-dimensional space. The more variables we have, the higher the dimensional space our observations are in. The ratio of our observations to the area of the dimensional space will continue to decrease. Thus, the amount of data available in that high dimension is sparse. This is the curse of dimensionality."
  },
  {
    "objectID": "14_dimension.html#techniques-of-dimensionality-reduction",
    "href": "14_dimension.html#techniques-of-dimensionality-reduction",
    "title": "\n14  Dimensionality Reduction\n",
    "section": "\n14.3 Techniques of Dimensionality Reduction",
    "text": "14.3 Techniques of Dimensionality Reduction\nThere are several techniques for reducing the dimensionality of data, broadly categorized into Feature Selection and Feature Extraction.\n\n14.3.1 Feature Selection\nFeature selection involves selecting a subset of the most relevant features for use in model construction. There are three main types of feature selection methods:\n\nFilter Methods:\nFilter Methods are among the first steps you can take in preprocessing your data for machine learning models. They are computationally less expensive than Wrapper and Embedded Methods because they do not involve training models as part of the feature selection process. Instead, they rely on general characteristics of the data, such as correlation coefficients, Chi-square tests, and mutual information.\nAdvantages:\n\n\nSpeed: They are fast and scalable to high-dimensional datasets because they evaluate features in isolation from the model.\n\nSimplicity: These methods are straightforward to understand and implement.\n\nModel Agnostic: They can be applied regardless of the choice of machine learning algorithm.\n\nDisadvantages:\n\nLess Accurate: They might not capture feature interactions well because they evaluate each feature independently.\nNo Model Context: They do not consider how features will interact when combined in a model, potentially overlooking combinations that would improve model performance.\n\nCommon Techniques in Filter Methods\n\nCorrelation Coefficient: This measures the linear relationship between two variables. Features with very low correlation to the target variable can be removed.\nChi-Square Test: This statistical test is used to determine if there is a significant association between two categorical variables. It can be used to select relevant features for classification problems.\nMutual Information: This measures the amount of information one can obtain from one variable through another. A higher value means more information shared, making it useful for feature selection.\nVariance Threshold: This method removes all features whose variance doesn’t meet some threshold. Since variables with a low variance are less likely to affect the target variable, they can be considered for removal.\n\nApplication of Filter Methods\nFilter Methods are widely used at the beginning stages of the feature selection process, especially when dealing with very high-dimensional data. They help in narrowing down the feature set to a more manageable size, which can then be further refined using more sophisticated techniques like Wrapper and Embedded Methods.\nExample: Batting data\nSuppose R is the response variable. We can examine the correlation between R and all other features.\n\ncorrelation_matrix &lt;- Batting |&gt; \n  filter(yearID==2004 & teamID==\"TEX\") |&gt; \n  select(R:BB) |&gt; \n  cor()\n\ncorrelations &lt;- correlation_matrix['R', ]\nprint(correlations)\n\n        R         H       X2B       X3B        HR       RBI        SB        CS \n1.0000000 0.9886300 0.9737135 0.8520469 0.9385321 0.9828114 0.6668248 0.6036253 \n       BB \n0.9459635 \n\n# Visualize the correlations for better understanding\ncorrelations %&gt;% as_tibble() %&gt;%\n  rownames_to_column(\"Feature\") %&gt;%\n  ggplot(aes(x = reorder(Feature, -value), y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(y = \"Correlation with R\", \n       x = \"Feature\", \n       title = \"Feature Correlation with R\")\n\n\n\n# Select features based on a correlation threshold, for example, features with absolute correlation &gt; 0.7\nrelevant_features &lt;- names(correlations[which(abs(correlations) &gt; 0.7)])\nprint(relevant_features)\n\n[1] \"R\"   \"H\"   \"X2B\" \"X3B\" \"HR\"  \"RBI\" \"BB\" \n\n\nWhile Filter Methods are an efficient way to reduce dimensionality, especially in the preliminary stages of model development, they should be part of a broader feature selection strategy that may include more sophisticated methods. Combining various methods thoughtfully can lead to the development of more accurate and robust predictive models.\n\nWrapper Methods:\nWrapper methods select features based on the performance of a predictive model, where features are added or removed according to their contribution to model accuracy. This approach differs from filter methods, which rely on the general characteristics of the data, and embedded methods, which perform feature selection as part of the model training process.\nExample: Applying Wrapper Methods to the Batting Dataset\nAssuming we are interested in modeling R based on the other batting statistics, we could use a wrapper method to select the most relevant features for predicting runs. This process involves iteratively adding or removing features based on their impact on the model’s predictive accuracy.\nWe use a stepwise regression approach, which considers both addition and removal of features based on their statistical significance to the model’s performance. The stepAIC function from the MASS package in R can perform this operation, aiming to minimize the Akaike Information Criterion (AIC) for model selection:\n\nlibrary(MASS)\nlibrary(tidyverse)\n\ndat = Batting |&gt; \n  filter(yearID==2004 & teamID==\"TEX\") |&gt; \n  dplyr::select(R:BB)\n\nfit &lt;- lm(R ~ ., data=dat)\nstepModel &lt;- stepAIC(fit, direction=\"both\", trace = 1)\n\nStart:  AIC=70.36\nR ~ H + X2B + X3B + HR + RBI + SB + CS + BB\n\n       Df Sum of Sq    RSS     AIC\n- SB    1     0.546 142.90  68.568\n- CS    1     0.987 143.34  68.731\n&lt;none&gt;              142.35  70.365\n- H     1     7.925 150.28  71.236\n- HR    1    22.283 164.63  76.072\n- X2B   1    32.174 174.53  79.165\n- RBI   1   101.383 243.73  96.867\n- X3B   1   106.221 248.57  97.909\n- BB    1   194.869 337.22 114.074\n\nStep:  AIC=68.57\nR ~ H + X2B + X3B + HR + RBI + CS + BB\n\n       Df Sum of Sq    RSS     AIC\n- CS    1      0.55 143.44  66.769\n&lt;none&gt;              142.90  68.568\n+ SB    1      0.55 142.35  70.365\n- H     1     28.94 171.84  76.341\n- HR    1     49.14 192.04  82.232\n- X2B   1    147.27 290.17 104.109\n- X3B   1    168.83 311.73 107.908\n- RBI   1    179.33 322.23 109.664\n- BB    1    341.15 484.05 131.230\n\nStep:  AIC=66.77\nR ~ H + X2B + X3B + HR + RBI + BB\n\n       Df Sum of Sq    RSS     AIC\n&lt;none&gt;              143.44  66.769\n+ CS    1      0.55 142.90  68.568\n+ SB    1      0.10 143.34  68.731\n- H     1     32.41 175.86  75.567\n- HR    1     61.07 204.51  83.567\n- X2B   1    146.73 290.18 102.110\n- RBI   1    181.01 324.45 108.027\n- X3B   1    199.91 343.36 111.030\n- BB    1    377.96 521.40 133.170\n\nsummary(stepModel)\n\n\nCall:\nlm(formula = R ~ H + X2B + X3B + HR + RBI + BB, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5557  0.0618  0.0618  0.5715  3.8356 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.06181    0.28073  -0.220  0.82671    \nH           -0.20661    0.06409  -3.224  0.00233 ** \nX2B          0.90354    0.13172   6.860 1.48e-08 ***\nX3B          5.17137    0.64587   8.007 2.89e-10 ***\nHR          -0.84258    0.19040  -4.425 5.87e-05 ***\nRBI          0.79409    0.10423   7.619 1.08e-09 ***\nBB           0.48002    0.04360  11.009 1.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.766 on 46 degrees of freedom\nMultiple R-squared:  0.997, Adjusted R-squared:  0.9966 \nF-statistic:  2570 on 6 and 46 DF,  p-value: &lt; 2.2e-16\n\n\nThis code starts with a model that includes all available features (except R, which is our target variable) and then iteratively adds or removes features to find a combination that offers the best balance between model complexity and predictive power, as measured by the AIC.\nEvaluating the Selected Features\nThe output of stepAIC will indicate which features have been selected as predictors for Runs. These features are deemed by the stepwise regression process as having significant predictive power for R, after considering the potential for overfitting (through AIC minimization).\nThis approach allows for an automated and data-driven selection of features, which can be especially useful when dealing with datasets with many variables. By focusing on the subset of features that contribute most to prediction accuracy, wrapper methods can help create more efficient and interpretable models.\n\nEmbedded Methods:\nEmbedded methods are particularly useful as they perform feature selection while the model is being trained, which can lead to a more optimal set of features for the prediction task at hand. Regularization methods like LASSO (Least Absolute Shrinkage and Selection Operator) are common examples of embedded methods because they both train the model and select features by penalizing the absolute size of the regression coefficients.\nEmbedded methods combine the qualities of filter and wrapper methods by performing feature selection as part of the model training process. This approach can lead to more accurate and efficient models because it considers the interaction between features and the model. One key advantage of embedded methods is their ability to capture complex interactions between features, which might be missed by filter methods.\nExample: Batting Data\nIn this example, we’ll use the Batting data to predict Runs. We’ll apply LASSO regression, an embedded method, using the tidymodels framework.\n\nlibrary(tidyverse)\n\nWe’ll use a LASSO regression model, which is suited for datasets with potentially correlated predictors and can help in feature selection by shrinking some coefficients to zero.\n\nlibrary(MultBiplotR)\nlibrary(tidymodels)\n\nlasso_spec &lt;- linear_reg(penalty = 0.1, mixture = 1)  |&gt; \n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"regression\")\n\nrecipe &lt;- recipe(R ~ ., data = dat)  |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  prep() \n\ndat_baked = recipe |&gt; bake(new_data = NULL)\n  \nlasso_fit &lt;- lasso_spec %&gt;%\n  fit(R~., data = dat_baked)\n\nlasso_fit |&gt; \n  tidy()\n\n# A tibble: 9 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   16.2       0.1\n2 H              0         0.1\n3 X2B            6.68      0.1\n4 X3B            6.05      0.1\n5 HR             0         0.1\n6 RBI           10.7       0.1\n7 SB             0         0.1\n8 CS             0.185     0.1\n9 BB             8.31      0.1\n\n\nThis process will provide us with an understanding of which features are most predictive of Runs, leveraging the embedded method’s ability to perform feature selection in conjunction with model training. By focusing on the subset of features that contribute most to prediction accuracy, we can create more efficient and interpretable models."
  },
  {
    "objectID": "14_dimension.html#feature-extraction",
    "href": "14_dimension.html#feature-extraction",
    "title": "\n14  Dimensionality Reduction\n",
    "section": "\n14.4 Feature Extraction",
    "text": "14.4 Feature Extraction\nFeature extraction transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear or nonlinear, with the transformed features being combinations of the original features. The most common feature extraction techniques include:\nPrincipal Component Analysis (PCA):\n\n\nPurpose: PCA reduces dimensionality by transforming the original variables into a new set of uncorrelated variables, called principal components, which are ordered by the amount of original variance they capture. The first principal component captures the most variance, the second captures the second most, and so on.\n\nSuppose we had two variables: height and hair color (some measure of darkness of hair) for a Native American tribe. Below is a scatterplot of the two variables:\n\nIt’s evident that in this tribe, the variation in hair color among individuals is minimal compared to the range of their heights. Therefore, height emerges as a more significant characteristic than hair color. Consequently, by incorporating only the height of individuals from this tribe as a feature in the dataset, we can preserve the majority of the relevant information.\nMost situation do not result in a scatterplot as we see above. Instead, you may see a situation as below.\n\n\n\n\n\nInstead of \\(X\\) or \\(y\\) having small variability, we can imagine a line drawn through the points and the variability of the points about that line is small.\n\n\n\n\n\nIf we rotate the plot so that the blue line is now the horizontal axis, the new axes can then be examined and we can use ony the axis that has the larger variability. These new axes are called the Principal Components.\n\n\n\n\n\nIn this example, the first principal component (PCA1) has large variability. the second principal component (PCA2) has small variability. So if we use only PCA1 as our feature, then we only lose a small amount of information in how the data varies when we do not select PCA2.\nPCA starts by calculating the covariance matrix of the data to understand how variables are related. It then computes the eigenvectors and eigenvalues of this covariance matrix. Eigenvectors determine the directions of the new space, and eigenvalues determine their magnitude. In essence, the eigenvectors with the highest eigenvalues are selected to form the new set of variables.\nLinear Discriminant Analysis (LDA):\n\n\nPurpose: LDA is a supervised dimensionality reduction technique used to find the linear combinations of features that best separate two or more classes of objects or events. The goal is to project the features in higher-dimensional space onto a lower-dimensional space with good class-separability in order to avoid overfitting (“curse of dimensionality”) and also reduce computational costs.\n\nHow it Works: LDA computes the directions (“linear discriminants”) that will represent the axes that maximize the separation between multiple classes. It takes the mean and variance of each class into account and seeks to reduce variance within each class while maximizing variance between the classes.\n\nApplications: LDA is particularly useful in the preprocessing steps for pattern classification and machine learning applications. Its application spans across various fields including face recognition, medical diagnosis, and any domain requiring classification tasks.\nt-Distributed Stochastic Neighbor Embedding (t-SNE):\n\n\nPurpose: t-SNE is a nonlinear technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It converts similarities between data points to joint probabilities and tries to minimize the divergence between these joint probabilities in the high-dimensional and low-dimensional space.\n\nHow it Works: t-SNE starts by calculating the probability that pairs of datapoints in the high-dimensional space are similar, then uses a gradient descent method to minimize the difference between this probability distribution and a similar distribution in the low-dimensional space.\n\nApplications: Because of its ability to preserve local structures and resolve clusters in a small area of the map, t-SNE is highly favored for visualizing high-dimensional data such as genetic data, image data, or text data.\n\nConsiderations When Choosing a Feature Extraction Technique:\n\nThe nature of the dataset: Is it linear or nonlinear? PCA and LDA assume linear relationships between variables, while t-SNE does not.\n\nSupervised vs. Unsupervised learning: PCA and t-SNE are unsupervised methods (they do not require labeled data), whereas LDA is supervised (requires labeled data).\n\nGoal of dimensionality reduction: Is the goal to improve visualization (t-SNE), to prepare for a classification task (LDA), or to reduce feature space while retaining variance (PCA)?\n\nComputational resources and dataset size: PCA and LDA are relatively more computationally efficient than t-SNE, especially for very large datasets.\nPCA Example: Batting Data\n\n# PCA with tidymodels\npca_recipe &lt;- recipe(~., data = dat) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  #Get enough PCs to get 95% of variance\n  step_pca(all_predictors(), threshold = .95) |&gt;  \n  prep()\n\n# Extract the PCA results\npca_results &lt;- bake(pca_recipe, new_data = NULL)\n\n# View the results\nprint(pca_results)\n\n# A tibble: 53 × 3\n      PC1     PC2      PC3\n    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 -1.25  -0.0800 -0.0304 \n 2 -0.696  0.283  -0.341  \n 3 -1.44  -0.0152 -0.0698 \n 4 -1.36  -0.0432  0.00207\n 5 -1.44  -0.0152 -0.0698 \n 6  2.45  -1.19    0.357  \n 7 -1.43  -0.0193 -0.0694 \n 8 -1.44  -0.0152 -0.0698 \n 9  7.22  -2.23    1.25   \n10 -1.44  -0.0152 -0.0698 \n# ℹ 43 more rows\n\ntidy(pca_recipe, number = 2, type = \"variance\") |&gt; \n  filter(terms == \"cumulative percent variance\") |&gt; \n  print()\n\n# A tibble: 9 × 4\n  terms                       value component id       \n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n1 cumulative percent variance  81.7         1 pca_zPW5q\n2 cumulative percent variance  92.9         2 pca_zPW5q\n3 cumulative percent variance  97.2         3 pca_zPW5q\n4 cumulative percent variance  98.8         4 pca_zPW5q\n5 cumulative percent variance  99.6         5 pca_zPW5q\n6 cumulative percent variance  99.9         6 pca_zPW5q\n7 cumulative percent variance 100.          7 pca_zPW5q\n8 cumulative percent variance 100.          8 pca_zPW5q\n9 cumulative percent variance 100           9 pca_zPW5q\n\n\nWe see that only three principal components are needed to capture 95% of the variability present in the nine predictor variables.\n\n# Visualize PCA results\npca_results %&gt;%\n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_point() +\n  theme_minimal() +\n  xlab(\"Principal Component 1\") +\n  ylab(\"Principal Component 2\")"
  },
  {
    "objectID": "15_SVM.html#linear-svm-classification",
    "href": "15_SVM.html#linear-svm-classification",
    "title": "\n15  Support Vector Machines\n",
    "section": "\n15.1 Linear SVM Classification",
    "text": "15.1 Linear SVM Classification\nLinear Support Vector Machine (SVM) Classification is a powerful method in machine learning that aims to find the optimal hyperplane which separates different classes in a dataset with the widest possible margin, hence ensuring robust classification boundaries. This section elaborates on the concept using the iris dataset as an example. This famous data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.\nLet’s first only consider the features petal length and petal width. We will also only focus on binary classification. First, let’s consider only the species setosa and versicolor.\n\ndata(iris)\nlibrary(tidyverse)\n# Filter the dataset\niris_filtered = iris |&gt;\n  filter(Species %in% c(\"versicolor\", \"setosa\")) |&gt; \n  mutate(Species = factor(Species))\n\niris_filtered |&gt; \n  ggplot(aes(x = Petal.Length, y = Petal.Width, color = Species))+\n  geom_point()\n\n\n\n\nIn this example, there is a clear separation of the two species in terms of the features petal length and petal width. A linear SVM tries to find a line that will separate the two classes. In this example, there are a lot of lines that would do this. However, if you have a line that is too close to the versicolor class, then when we have new observations that we want to predict, we are more likely to have some that would cross that line and be missclassified as setosa.\nIf we were to move to line too close to the setosa group, then we are more likely to have a new observation that would be missclassified as versicolor. So we need a line that will be somewhere in the “middle”. That is, we want the line that stays as far away from the closest training instances as possible.\nAn SVM classifier can be visualized as creating the broadest possible “street” (denoted by parallel dashed lines) separating the classes, a process known as large margin classification.\n\n\n\n\n\nThere are other lines that would give us this “street” that would separate the classes, however, the margin (width) of this street will be smaller like below.\n\n\n\n\n\nWhen we have a narrower street, then we have a higher chance of missclassifying observations when we have new data. Thus, our goal is to make the street as wide as possible. The middle solid line is then the decision boundary. Any new observation on the side closest to the blue points would be classified as versicolor and any observation on the other side of that line would be classified as setosa.\nNote that incorporating additional training observations away from the “street” does not impact the decision boundary; it is entirely shaped by (or “supported” by) the samples situated on the street’s edge. These particular samples are known as the support vectors.\nSoft Margin Classification\nImagine we have a situation where there is no clear separation in the two classes. Or perhaps we have an outlier like in the example below.\n\n\n\n\n\nWe would not be able to find a “street” that would separate these classes. When we enforce a rule requiring all data points to be off the street and positioned on the correct side, this approach is termed hard margin classification. However, hard margin classification encounters two primary challenges. The first challenge is that it demands the data to be linearly separable for effective application. The second challenge is its vulnerability to outliers, meaning that even a few anomalous data points can significantly impact the classification outcome.\nTo circumvent these problems, it’s beneficial to adopt a more adaptable approach. The goal is to strike an optimal balance between maximizing the width of the street and minimizing margin violations, which occur when observations land in the middle of the street or on the incorrect side. This approach is known as soft margin classification.\nThe balance between the width of the street and the margin violations is governed by the hyperparemeter \\(C\\). If \\(C\\) is to large, the street becomes too narrow but with few margin violations. If \\(C\\) is too small, then the street is too wide which results in a lot of margin violations. We will need to tune this hyperparameter. That is, try different values of \\(C\\) and find which one does the best job at predicting new observations.\n\n15.1.1 Feature Scaling\nIn the context of SVMs, the importance of feature scaling cannot be overstated. SVMs are sensitive to the scale of the features because the aim is to maximize the margin between classes. If one feature dominates because of its larger scale, the SVM might not perform effectively. Hence, preprocessing steps involving normalization or standardization of features are crucial.\nExample: Linear SVM Classification with the Iris Dataset Using tidymodels\nTo illustrate linear SVM classification using the tidymodels framework in R, we will focus on classifying two species of the iris dataset: virginica and versicolor.\nLoading the Dataset and Preprocessing\nFirst, we load the iris dataset and filter it to only include the two species of interest: virginica and versicolor (instead of setosa and versicolor above). We then split the dataset into a training set and a testing set to evaluate the model’s performance. We also will setup the training data to do cross validation to tune the hyperparameter $C4.\n\nlibrary(tidymodels)\nlibrary(tidyverse)\n\ndata(iris)\n# Filter the dataset\niris_filtered = iris |&gt;\n  filter(Species %in% c(\"versicolor\", \"virginica\")) |&gt; \n  mutate(Species = factor(Species))\n\n# Create a data split\nset.seed(123)\ndata_split = initial_split(iris_filtered, prop = 0.75)\ntrain_data = training(data_split)\ntest_data = testing(data_split)\n\ndat_folds = vfold_cv(train_data, v = 5, strata = Species)\n\nBelow is a plot of these species in the training data in terms of petal length and petal width. When we fit this model, we will use all four features (sepal width, sepal length, petal width, and petal length).\n\ntrain_data |&gt; \nggplot(aes(x = Petal.Length,y = Petal.Width, color = Species))+\n  geom_point()\n\n\n\n\nWhen we incorporate all four features, we can do a better job at predicting, however, it becomes difficult to visualize what the “street” look like. Instead of lines, the street is made up of hyperplanes which we cannot visualize.\nSpecifying the Model and Preprocessing Steps\nWe specify a linear SVM model using svm_linear() from the parsnip package, which is part of the tidymodels framework. The \\(C\\) hyperparameter is called cost in the svm_linear function. We will tell the model that we want to tune this hyperparameter. Additionally, we define preprocessing steps, including feature scaling, to ensure that all features contribute equally to the model.\n\n# Specify the model\nsvm_model = svm_linear(cost = tune()) |&gt;\n  set_engine(\"kernlab\") |&gt;\n  set_mode(\"classification\")\n\n# Preprocessing\nrecipe = recipe(Species ~ ., data = train_data) |&gt;\n  step_normalize(all_predictors())\n\nTune the Model\nWe will setup a grid of possible values of \\(C\\) to try. The function grid_regular checks from -10 to 5 on the log base 2 scale. That is, it checks from \\(2^{-10}=0.00098\\) to \\(2^5 = 32\\). The values of levels tells the function how many equally spaced values on this interval to check.\nWe then pass this tuning grid along with the 5-folded data to a workflow that includes the recipe and the model. We will find the value of \\(C\\) that maximizes accuracy\n\n#setup the possible values of C to check\ntuning_grid = grid_regular(\n  cost(),\n  levels = 20\n)\n\ntune_results = tune_grid(\n  object = workflow() |&gt;\n    add_recipe(recipe) |&gt;\n    add_model(svm_model),\n  resamples = dat_folds,\n  grid = tuning_grid\n)\n\nbest_params = select_best(tune_results, metric = \"accuracy\")\nbest_params\n\n# A tibble: 1 × 2\n   cost .config              \n  &lt;dbl&gt; &lt;chr&gt;                \n1 0.402 Preprocessor1_Model12\n\n\nTraining the Model\nWith the model and preprocessing steps specified, we can now train the SVM model on the training data.\n\n# Workflow\nfitted_model = finalize_workflow(\n  workflow() |&gt;\n    add_recipe(recipe) |&gt;\n    add_model(svm_model),\n  best_params\n) |&gt;\n  fit(data = train_data)\n\n Setting default kernel parameters  \n\n\nEvaluating the Model\nFinally, we evaluate the model’s performance on the testing set to understand its effectiveness in classifying the two species.\n\n# Predictions\npredictions = predict(fitted_model, new_data = test_data) |&gt;\n  bind_cols(test_data)\n\n# Evaluate\nmetrics = metrics(predictions, truth = Species, estimate = .pred_class)\nconf_mat = conf_mat(predictions, truth = Species, estimate = .pred_class)\n\nprint(metrics)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.96 \n2 kap      binary         0.920\n\nprint(conf_mat)\n\n            Truth\nPrediction   versicolor virginica\n  versicolor         12         0\n  virginica           1        12\n\n\nThis example demonstrates how to implement linear SVM classification using the tidymodels framework in R, focusing on the nuances of dealing with a subset of the iris dataset. Through this example, we see the robustness of SVM in creating a model that effectively separates the two chosen species based on their feature measurements, underscoring the importance of feature scaling and model specification in the process."
  },
  {
    "objectID": "15_SVM.html#nonlinear-svm-classification",
    "href": "15_SVM.html#nonlinear-svm-classification",
    "title": "\n15  Support Vector Machines\n",
    "section": "\n15.2 Nonlinear SVM Classification",
    "text": "15.2 Nonlinear SVM Classification\nFor datasets that are not linearly separable, SVMs can be extended to perform nonlinear classification. By adding polynomial features or using similarity features, datasets can often be transformed into a linearly separable form. We will discuss methods to handle nonlinear datasets, including using polynomial features and the kernel trick, which allows for operating in a high-dimensional space without explicitly computing the coordinates of the data points in that space.\n\n15.2.1 Introduction to Kernel Trick\nThe kernel trick is central to enabling SVMs to perform complex nonlinear classifications. This technique involves mapping input features into high-dimensional spaces where the data points that are not linearly separable in the original space might become linearly separable. Crucially, the kernel trick does this mapping without explicitly computing the coordinates in the high-dimensional space, thereby avoiding the computational complexity that such calculations would entail. Common kernels include:\n\n\nPolynomial Kernel: Adds polynomial features of a given degree. This kernel is powerful for capturing the interaction between features up to a specific degree.\n\nRadial Basis Function (RBF) or Gaussian Kernel: Considers all possible polynomials of all degrees, giving more weight to the features that are closer to the target. This kernel is particularly effective for cases where the relationship between the class boundaries is not only nonlinear but also varies in complexity across the dataset.\n\nSigmoid Kernel: Mirrors the use of sigmoid functions in neural networks and can transform the feature space in ways that are beneficial for certain types of datasets.\n\nAdvantages of Kernel SVM\nThe primary advantage of using kernel SVMs lies in their flexibility and ability to handle real-world data that often display complex patterns and nonlinear relationships. Kernel SVMs can capture intricate structures without requiring a massive increase in computational resources typically associated with high-dimensional space mapping.\nChoosing the Right Kernel\nThe choice of kernel significantly affects the model’s performance. No single kernel universally outperforms others across all tasks; the decision is highly data-dependent. Cross-validation can help in selecting the best kernel and its parameters for a given dataset.\n\n\nPolynomial kernels are suitable when the relationship between variables is expected to be polynomial.\n\nRBF kernels are a good default when there is little prior knowledge about the data.\n\nSigmoid kernels can be useful but are less commonly used than RBF or polynomial kernels.\n\n15.2.2 Parameter Tuning in Kernel SVM\nTwo critical parameters in kernel SVMs are \\(C\\) (the regularization parameter) and the kernel-specific parameter (like degree in polynomial kernels or \\(\\gamma\\) in RBF kernels). The parameter \\(C\\) controls the trade-off between achieving a low training error and a low testing error (generalization), whereas kernel-specific parameters control the shape of the boundary.\nBelow are examples of different hyperparameter values using RBF kernels.\n\n\n15.2.3 Implementing Nonlinear SVM\nWhen implementing nonlinear SVMs, the process typically involves several key steps:\n\nFeature Preprocessing: Scaling features to a similar scale is crucial because kernel SVMs are sensitive to the feature scales.\nModel Selection: Choosing between different SVM kernels based on the problem at hand and the nature of the data.\nCross-Validation: Employing cross-validation techniques to fine-tune hyperparameters (such as \\(C\\), kernel parameters like degree for polynomial kernel, or \\(\\gamma\\) for the RBF kernel) is essential for balancing the model’s complexity with its ability to generalize to unseen data.\nEvaluation: Assessing the model’s performance using appropriate metrics (like accuracy, precision, recall, F1 score for classification tasks) on a validation set not seen by the model during the training phase.\nApplication: Once tuned and evaluated, the model can be applied to new, unseen data for prediction tasks.\n\nChallenges and Considerations\nWhile nonlinear SVMs are powerful, they come with their own set of challenges and considerations:\n\n\nComputation Cost: The computational complexity can be higher than for linear models, especially for large datasets and complex kernels.\n\nModel Interpretability: The decision boundaries created by nonlinear SVMs can be difficult to interpret compared to linear SVMs.\n\nOverfitting Risk: There is a higher risk of overfitting, especially with very flexible models like those using high-degree polynomial kernels or small \\(\\gamma\\) values in RBF kernels. Regularization and proper parameter tuning are vital to mitigate this risk."
  },
  {
    "objectID": "15_SVM.html#svm-regression",
    "href": "15_SVM.html#svm-regression",
    "title": "\n15  Support Vector Machines\n",
    "section": "\n15.3 SVM Regression",
    "text": "15.3 SVM Regression\nSVMs can also be applied to regression problems by reversing the objective: instead of trying to maximize the margin while keeping the instances outside the margin, SVM regression attempts to fit as many instances as possible within the margin while limiting margin violations. This approach is known as (\\(\\epsilon\\))-insensitive loss, where the model tries to find a line that captures as many instances as possible within a specified margin.\nHere’s an example demonstrating how to use SVM Regression with the tidymodels framework in R, specifically applied to the diamonds dataset. We’ll predict the price of diamonds based on their features such as carat, cut, color, and clarity.\nPrepare the Data\nThe diamonds dataset is available in the ggplot2 package. We’ll use a subset of the dataset to make the training process faster for this example.\n\ndata(\"diamonds\", package = \"ggplot2\")\nset.seed(123)\ndiamonds_sample = diamonds |&gt; sample_n(size = 2000)\n\n# Split the data into training and testing sets\nsplit = initial_split(diamonds_sample, prop = 0.75)\ntrain_data = training(split)\ntest_data = testing(split)\n\nDefine the Recipe\n\nrec = recipe(price ~ carat + cut + color + clarity, data = train_data) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_normalize(all_numeric_predictors())\n\nSpecify the Model\nWe’ll use an SVM model for regression. The svm_rbf() function from the parsnip package is suitable for this task. The RBF (Radial Basis Function) kernel is commonly used for non-linear regression problems.\n\nsvm_mod = svm_rbf(cost = tune(), rbf_sigma = tune()) |&gt;\n  set_engine(\"kernlab\") |&gt;\n  set_mode(\"regression\")\n\nTune the Model\nWe’ll use cross-validation to tune the hyperparameters of the model.\n\n# Create a 5-fold cross-validation object\nfolds = vfold_cv(train_data, v = 5)\n\n# Create a grid for tuning\ngrid = grid_regular(cost(range = c(-5, 2)),\n                     rbf_sigma(range = c(-5, 2)),\n                     levels = 5)\n\n# Tune the model\ntune_res = tune_grid(\n  object = workflow() |&gt;\n        add_recipe(rec) |&gt;\n        add_model(svm_mod),\n  resamples = folds,\n  grid = grid\n)\n\n# Select the best hyperparameters\nbest_params = select_best(tune_res, metric=\"rmse\")\nbest_params\n\n# A tibble: 1 × 3\n   cost rbf_sigma .config              \n  &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                \n1     4    0.0316 Preprocessor1_Model15\n\n\nFit the Final Model\nAfter finding the best hyperparameters, we fit the final model to the training data.\n\nfinal_svm = finalize_workflow(\n  workflow() |&gt;\n    add_recipe(rec) |&gt;\n    add_model(svm_mod),\n  best_params\n) |&gt;\n  fit(data = train_data)\n\nEvaluate the Model\nFinally, we evaluate the model’s performance on the testing set.\n\npredictions = predict(final_svm, new_data = test_data) |&gt;\n  bind_cols(test_data)\n\nmetrics = metrics(predictions, truth = price, estimate = .pred)\nmetrics\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     780.   \n2 rsq     standard       0.962\n3 mae     standard     491."
  }
]