[
  {
    "objectID": "14_dimension.html#introduction",
    "href": "14_dimension.html#introduction",
    "title": "\n14  Dimensionality Reduction\n",
    "section": "\n14.1 Introduction",
    "text": "14.1 Introduction\nIn the realm of statistics and data science, the ability to effectively analyze and draw insights from data is paramount. As we venture into the era of big data, we encounter datasets of increasing complexity and size. These datasets often comprise a vast number of variables, a situation described as high dimensionality. Understanding the concept of dimensionality reduction is essential, not just as an abstract mathematical idea but as a practical tool for making sense of complex data.\nDimensionality reduction sits at the heart of sports analytics, serving as a bridge between raw data and actionable insights. It addresses several critical challenges in data analysis, including the curse of dimensionality, noise in the dataset, and the difficulties involved in visualizing multidimensional data. By simplifying the data without significant loss of information, dimensionality reduction techniques enable us to build models that are not only more efficient but also more interpretable.\nThe study of dimensionality reduction offers a glimpse into the interdisciplinary nature of data science, where statistics, computer science, and domain expertise converge. This area highlights the importance of understanding both the theoretical foundations and the practical applications of statistical methods. While the mathematical underpinnings, such as linear algebra, are crucial, the focus here is on grasping the conceptual framework and the impact of dimensionality reduction on data analysis.\nThe introduction to dimensionality reduction begins with the rationale behind it. As datasets grow in size and complexity, the limitations of traditional analytical tools become apparent. The curse of dimensionality, a phenomenon where the data space expands so much that our data becomes sparse, affects not only the computational feasibility of models but also their performance. Reducing the number of input variables helps mitigate these issues, making models simpler, faster, and more generalizable.\nMoreover, in a practical setting, the reduction of dimensionality can be pivotal for noise reduction and visualization. By filtering out irrelevant or redundant features, we improve the model’s accuracy and reliability. Similarly, the transformation of high-dimensional data into a more manageable form allows for effective visualization, which is indispensable for data exploration and hypothesis generation.\nDimensionality reduction techniques, categorized into feature selection and feature extraction, offer a toolkit for addressing these challenges. Feature selection methods focus on identifying the most relevant features for the models. On the other hand, feature extraction techniques like Principal Component Analysis (PCA), transform the original features into a new set of variables that better capture the underlying structure of the data."
  },
  {
    "objectID": "14_dimension.html#reasons-for-dimensionality-reduction",
    "href": "14_dimension.html#reasons-for-dimensionality-reduction",
    "title": "\n14  Dimensionality Reduction\n",
    "section": "\n14.2 Reasons for Dimensionality Reduction",
    "text": "14.2 Reasons for Dimensionality Reduction\n\n14.2.1 Curse of Dimensionality: The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. It’s essential in understanding how dimensionality affects data analysis, leading to specific issues like data sparsity and increased computational complexity.\nUnderstanding the Curse of Dimensionality\nWhen the dimensionality increases, the volume of the space increases so rapidly that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Additionally, high-dimensional datasets are often accompanied by increased computational complexity and a greater chance of overfitting, making models less generalizable to new data.\nExample using the Batting dataset\nThe Batting dataset in the Lahman library consists of batting statistics for MLB players.\nLoading and Exploring the Data\n\nlibrary(Lahman)\nlibrary(tidyverse)\nlibrary(GGally)\ndata(\"Batting\")\n\nBelow is a scatterplot matrix of the pairs of 9 of the quantitative features for the Texas Rangers during the 2004 season.\n\nBatting |&gt; \n  filter(yearID==2004 & teamID==\"TEX\") |&gt; \n  select(R:BB) |&gt; \n  ggpairs()\n\n\n\n\nWith so many variables, it becomes cumbersome to visualize the relationships between the variables. The above scatterplot matrix shows the scatterplots of the 36 pairs of variables when you have nine features. It becomes difficult to examine all of these scatterplots. These plots also do not show any relationships between three variables at a time, and so forth.\nIn addition to visualization, the curse of dimenstionality also implies the sparseness of the data when we have more variables.\nLet’s first examine the variable R. Below is a dotplot for this variable.\n\nBatting |&gt; \n  filter(yearID==2004 & teamID==\"TEX\") |&gt; \n  ggplot(aes(x = R, y = 0))+\n  geom_point()+\n  ylab(\"\")+\n  theme(\n    axis.ticks.y=element_blank(),\n    axis.text.y=element_blank()\n    )\n\n\n\n\nNote that the min value of R is 0 and the max value is 114. There are 53 observations so we can think of the data for this variable as 53 pieces of information in that variable’s dimension (the \\(x\\) dimenstion on the plot above). Let’s calculate the ratio of information to the dimensional space:\n\\[\n\\frac{53}{114-0} = 0.4649\n\\]\nLet’s now examine another variable along with R. Below is the scatterplot of R and RBI\n\nBatting |&gt; \n  filter(yearID==2004 & teamID==\"TEX\") |&gt;\n  ggplot(aes(x = R, y = RBI))+\n  geom_point()\n\n\n\n\nFor RBI, the range of values are min = 0 and max = 112. The total dimensional space that these two variables take is \\[\n\\begin{align*}\n\\text{dimensional space for R}\\times\\text{dimensional space for RBI} &= 114 \\times 112\\\\\n&= 12768\n\\end{align*}\n\\]\nWe still only have 25 observations. The 53 pieces of information that we have in this two-dimensional space gives us the ratio \\[\n\\frac{53}{12768} = 0.0042\n\\]\nSo in two-dimensional space, the amount of data we have is a much lower ratio of the space than when we had in only one dimension.\nLet’s now add in a third variable BB. Note that the dimensional space for BB is min = 0 and max = 75. The size of the total dimensional space is \\[\n\\begin{align*}\n\\text{dim. space for R}\\times\\text{dim. space for RBI}\\times\\text{dim. space for BB} &= 114 \\times 112 \\times 75\\\\\n&= 957600\n\\end{align*}\n\\]\nAgain, we still only have 53 observations. So our 53 pieces of information only take up a ratio of \\[\n\\frac{53}{957600}=0.00006\n\\] of the 3-dimensional space. The more variables we have, the higher the dimensional space our observations are in. The ratio of our observations to the area of the dimensional space will continue to decrease. Thus, the amount of data available in that high dimension is sparse. This is the curse of dimensionality."
  },
  {
    "objectID": "14_dimension.html#techniques-of-dimensionality-reduction",
    "href": "14_dimension.html#techniques-of-dimensionality-reduction",
    "title": "\n14  Dimensionality Reduction\n",
    "section": "\n14.3 Techniques of Dimensionality Reduction",
    "text": "14.3 Techniques of Dimensionality Reduction\nThere are several techniques for reducing the dimensionality of data, broadly categorized into Feature Selection and Feature Extraction.\n\n14.3.1 Feature Selection\nFeature selection involves selecting a subset of the most relevant features for use in model construction. There are three main types of feature selection methods:\n\nFilter Methods: Filter Methods are among the first steps you can take in preprocessing your data for machine learning models. They are computationally less expensive than Wrapper and Embedded Methods because they do not involve training models as part of the feature selection process. Instead, they rely on general characteristics of the data, such as correlation coefficients, Chi-square tests, and mutual information.\nAdvantages:\n\n\nSpeed: They are fast and scalable to high-dimensional datasets because they evaluate features in isolation from the model.\n\nSimplicity: These methods are straightforward to understand and implement.\n\nModel Agnostic: They can be applied regardless of the choice of machine learning algorithm.\n\nDisadvantages:\n\nLess Accurate: They might not capture feature interactions well because they evaluate each feature independently.\nNo Model Context: They do not consider how features will interact when combined in a model, potentially overlooking combinations that would improve model performance.\n\nCommon Techniques in Filter Methods\n\nCorrelation Coefficient: This measures the linear relationship between two variables. Features with very low correlation to the target variable can be removed.\nChi-Square Test: This statistical test is used to determine if there is a significant association between two categorical variables. It can be used to select relevant features for classification problems.\nMutual Information: This measures the amount of information one can obtain from one variable through another. A higher value means more information shared, making it useful for feature selection.\nVariance Threshold: This method removes all features whose variance doesn’t meet some threshold. Since variables with a low variance are less likely to affect the target variable, they can be considered for removal.\n\nApplication of Filter Methods\nFilter Methods are widely used at the beginning stages of the feature selection process, especially when dealing with very high-dimensional data. They help in narrowing down the feature set to a more manageable size, which can then be further refined using more sophisticated techniques like Wrapper and Embedded Methods.\nExample: Batting data\nSuppose R is the response variable. We can examine the correlation between R and all other features.\n\ncorrelation_matrix &lt;- Batting |&gt; \n  filter(yearID==2004 & teamID==\"TEX\") |&gt; \n  select(R:BB) |&gt; \n  cor()\n\ncorrelations &lt;- correlation_matrix['R', ]\nprint(correlations)\n\n        R         H       X2B       X3B        HR       RBI        SB        CS \n1.0000000 0.9886300 0.9737135 0.8520469 0.9385321 0.9828114 0.6668248 0.6036253 \n       BB \n0.9459635 \n\n# Visualize the correlations for better understanding\ncorrelations %&gt;% as_tibble() %&gt;%\n  rownames_to_column(\"Feature\") %&gt;%\n  ggplot(aes(x = reorder(Feature, -value), y = value)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(y = \"Correlation with R\", \n       x = \"Feature\", \n       title = \"Feature Correlation with R\")\n\n\n\n# Select features based on a correlation threshold, for example, features with absolute correlation &gt; 0.7\nrelevant_features &lt;- names(correlations[which(abs(correlations) &gt; 0.7)])\nprint(relevant_features)\n\n[1] \"R\"   \"H\"   \"X2B\" \"X3B\" \"HR\"  \"RBI\" \"BB\" \n\n\nWhile Filter Methods are an efficient way to reduce dimensionality, especially in the preliminary stages of model development, they should be part of a broader feature selection strategy that may include more sophisticated methods. Combining various methods thoughtfully can lead to the development of more accurate and robust predictive models.\n\nWrapper Methods: Wrapper methods select features based on the performance of a predictive model, where features are added or removed according to their contribution to model accuracy. This approach differs from filter methods, which rely on the general characteristics of the data, and embedded methods, which perform feature selection as part of the model training process.\nExample: Applying Wrapper Methods to the Batting Dataset\nAssuming we are interested in modeling R based on the other batting statistics, we could use a wrapper method to select the most relevant features for predicting runs. This process involves iteratively adding or removing features based on their impact on the model’s predictive accuracy.\nWe use a stepwise regression approach, which considers both addition and removal of features based on their statistical significance to the model’s performance. The stepAIC function from the MASS package in R can perform this operation, aiming to minimize the Akaike Information Criterion (AIC) for model selection:\n\nlibrary(MASS)\nlibrary(tidyverse)\n\ndat = Batting |&gt; \n  filter(yearID==2004 & teamID==\"TEX\") |&gt; \n  dplyr::select(R:BB)\n\nfit &lt;- lm(R ~ ., data=dat)\nstepModel &lt;- stepAIC(fit, direction=\"both\", trace = 1)\n\nStart:  AIC=70.36\nR ~ H + X2B + X3B + HR + RBI + SB + CS + BB\n\n       Df Sum of Sq    RSS     AIC\n- SB    1     0.546 142.90  68.568\n- CS    1     0.987 143.34  68.731\n&lt;none&gt;              142.35  70.365\n- H     1     7.925 150.28  71.236\n- HR    1    22.283 164.63  76.072\n- X2B   1    32.174 174.53  79.165\n- RBI   1   101.383 243.73  96.867\n- X3B   1   106.221 248.57  97.909\n- BB    1   194.869 337.22 114.074\n\nStep:  AIC=68.57\nR ~ H + X2B + X3B + HR + RBI + CS + BB\n\n       Df Sum of Sq    RSS     AIC\n- CS    1      0.55 143.44  66.769\n&lt;none&gt;              142.90  68.568\n+ SB    1      0.55 142.35  70.365\n- H     1     28.94 171.84  76.341\n- HR    1     49.14 192.04  82.232\n- X2B   1    147.27 290.17 104.109\n- X3B   1    168.83 311.73 107.908\n- RBI   1    179.33 322.23 109.664\n- BB    1    341.15 484.05 131.230\n\nStep:  AIC=66.77\nR ~ H + X2B + X3B + HR + RBI + BB\n\n       Df Sum of Sq    RSS     AIC\n&lt;none&gt;              143.44  66.769\n+ CS    1      0.55 142.90  68.568\n+ SB    1      0.10 143.34  68.731\n- H     1     32.41 175.86  75.567\n- HR    1     61.07 204.51  83.567\n- X2B   1    146.73 290.18 102.110\n- RBI   1    181.01 324.45 108.027\n- X3B   1    199.91 343.36 111.030\n- BB    1    377.96 521.40 133.170\n\nsummary(stepModel)\n\n\nCall:\nlm(formula = R ~ H + X2B + X3B + HR + RBI + BB, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5557  0.0618  0.0618  0.5715  3.8356 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.06181    0.28073  -0.220  0.82671    \nH           -0.20661    0.06409  -3.224  0.00233 ** \nX2B          0.90354    0.13172   6.860 1.48e-08 ***\nX3B          5.17137    0.64587   8.007 2.89e-10 ***\nHR          -0.84258    0.19040  -4.425 5.87e-05 ***\nRBI          0.79409    0.10423   7.619 1.08e-09 ***\nBB           0.48002    0.04360  11.009 1.75e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.766 on 46 degrees of freedom\nMultiple R-squared:  0.997, Adjusted R-squared:  0.9966 \nF-statistic:  2570 on 6 and 46 DF,  p-value: &lt; 2.2e-16\n\n\nThis code starts with a model that includes all available features (except R, which is our target variable) and then iteratively adds or removes features to find a combination that offers the best balance between model complexity and predictive power, as measured by the AIC.\nEvaluating the Selected Features\nThe output of stepAIC will indicate which features have been selected as predictors for Runs. These features are deemed by the stepwise regression process as having significant predictive power for R, after considering the potential for overfitting (through AIC minimization).\nThis approach allows for an automated and data-driven selection of features, which can be especially useful when dealing with datasets with many variables. By focusing on the subset of features that contribute most to prediction accuracy, wrapper methods can help create more efficient and interpretable models.\n\nEmbedded Methods: Embedded methods are particularly useful as they perform feature selection while the model is being trained, which can lead to a more optimal set of features for the prediction task at hand. Regularization methods like LASSO (Least Absolute Shrinkage and Selection Operator) are common examples of embedded methods because they both train the model and select features by penalizing the absolute size of the regression coefficients.\nEmbedded methods combine the qualities of filter and wrapper methods by performing feature selection as part of the model training process. This approach can lead to more accurate and efficient models because it considers the interaction between features and the model. One key advantage of embedded methods is their ability to capture complex interactions between features, which might be missed by filter methods.\nExample: Batting Data\nIn this example, we’ll use the Batting data to predict Runs. We’ll apply LASSO regression, an embedded method, using the tidymodels framework.\n\nlibrary(tidyverse)\n\nWe’ll use a LASSO regression model, which is suited for datasets with potentially correlated predictors and can help in feature selection by shrinking some coefficients to zero.\n\nlibrary(MultBiplotR)\nlibrary(tidymodels)\n\nlasso_spec &lt;- linear_reg(penalty = 0.1, mixture = 1)  |&gt; \n  set_engine(\"glmnet\") |&gt; \n  set_mode(\"regression\")\n\nrecipe &lt;- recipe(R ~ ., data = dat)  |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  prep() \n\ndat_baked = recipe |&gt; bake(new_data = NULL)\n  \nlasso_fit &lt;- lasso_spec %&gt;%\n  fit(R~., data = dat_baked)\n\nlasso_fit |&gt; \n  tidy()\n\n# A tibble: 9 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   16.2       0.1\n2 H              0         0.1\n3 X2B            6.68      0.1\n4 X3B            6.05      0.1\n5 HR             0         0.1\n6 RBI           10.7       0.1\n7 SB             0         0.1\n8 CS             0.185     0.1\n9 BB             8.31      0.1\n\n\nThis process will provide us with an understanding of which features are most predictive of Runs, leveraging the embedded method’s ability to perform feature selection in conjunction with model training. By focusing on the subset of features that contribute most to prediction accuracy, we can create more efficient and interpretable models."
  },
  {
    "objectID": "14_dimension.html#feature-extraction",
    "href": "14_dimension.html#feature-extraction",
    "title": "\n14  Dimensionality Reduction\n",
    "section": "\n14.4 Feature Extraction",
    "text": "14.4 Feature Extraction\nFeature extraction transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear or nonlinear, with the transformed features being combinations of the original features. The most common feature extraction techniques include:\nPrincipal Component Analysis (PCA):\n\n\nPurpose: PCA reduces dimensionality by transforming the original variables into a new set of uncorrelated variables, called principal components, which are ordered by the amount of original variance they capture. The first principal component captures the most variance, the second captures the second most, and so on.\n\nSuppose we had two variables: height and hair color (some measure of darkness of hair) for a Native American tribe. Below is a scatterplot of the two variables:\n\nIt’s evident that in this tribe, the variation in hair color among individuals is minimal compared to the range of their heights. Therefore, height emerges as a more significant characteristic than hair color. Consequently, by incorporating only the height of individuals from this tribe as a feature in the dataset, we can preserve the majority of the relevant information.\nMost situation do not result in a scatterplot as we see above. Instead, you may see a situation as below.\n\n\n\n\n\nInstead of \\(X\\) or \\(y\\) having small variability, we can imagine a line drawn through the points and the variability of the points about that line is small.\n\n\n\n\n\nIf we rotate the plot so that the blue line is now the horizontal axis, the new axes can then be examined and we can use ony the axis that has the larger variability. These new axes are called the Principal Components.\n\n\n\n\n\nIn this example, the first principal component (PCA1) has large variability. the second principal component (PCA2) has small variability. So if we use only PCA1 as our feature, then we only lose a small amount of information in how the data varies when we do not select PCA2.\nPCA starts by calculating the covariance matrix of the data to understand how variables are related. It then computes the eigenvectors and eigenvalues of this covariance matrix. Eigenvectors determine the directions of the new space, and eigenvalues determine their magnitude. In essence, the eigenvectors with the highest eigenvalues are selected to form the new set of variables.\nLinear Discriminant Analysis (LDA):\n\n\nPurpose: LDA is a supervised dimensionality reduction technique used to find the linear combinations of features that best separate two or more classes of objects or events. The goal is to project the features in higher-dimensional space onto a lower-dimensional space with good class-separability in order to avoid overfitting (“curse of dimensionality”) and also reduce computational costs.\n\nHow it Works: LDA computes the directions (“linear discriminants”) that will represent the axes that maximize the separation between multiple classes. It takes the mean and variance of each class into account and seeks to reduce variance within each class while maximizing variance between the classes.\n\nApplications: LDA is particularly useful in the preprocessing steps for pattern classification and machine learning applications. Its application spans across various fields including face recognition, medical diagnosis, and any domain requiring classification tasks.\nt-Distributed Stochastic Neighbor Embedding (t-SNE):\n\n\nPurpose: t-SNE is a nonlinear technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It converts similarities between data points to joint probabilities and tries to minimize the divergence between these joint probabilities in the high-dimensional and low-dimensional space.\n\nHow it Works: t-SNE starts by calculating the probability that pairs of datapoints in the high-dimensional space are similar, then uses a gradient descent method to minimize the difference between this probability distribution and a similar distribution in the low-dimensional space.\n\nApplications: Because of its ability to preserve local structures and resolve clusters in a small area of the map, t-SNE is highly favored for visualizing high-dimensional data such as genetic data, image data, or text data.\n\nConsiderations When Choosing a Feature Extraction Technique:\n\nThe nature of the dataset: Is it linear or nonlinear? PCA and LDA assume linear relationships between variables, while t-SNE does not.\n\nSupervised vs. Unsupervised learning: PCA and t-SNE are unsupervised methods (they do not require labeled data), whereas LDA is supervised (requires labeled data).\n\nGoal of dimensionality reduction: Is the goal to improve visualization (t-SNE), to prepare for a classification task (LDA), or to reduce feature space while retaining variance (PCA)?\n\nComputational resources and dataset size: PCA and LDA are relatively more computationally efficient than t-SNE, especially for very large datasets.\nPCA Example: Batting Data\n\n# PCA with tidymodels\npca_recipe &lt;- recipe(~., data = dat) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  #Get enough PCs to get 95% of variance\n  step_pca(all_predictors(), threshold = .95) |&gt;  \n  prep()\n\n# Extract the PCA results\npca_results &lt;- bake(pca_recipe, new_data = NULL)\n\n# View the results\nprint(pca_results)\n\n# A tibble: 53 × 3\n      PC1     PC2      PC3\n    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 -1.25  -0.0800 -0.0304 \n 2 -0.696  0.283  -0.341  \n 3 -1.44  -0.0152 -0.0698 \n 4 -1.36  -0.0432  0.00207\n 5 -1.44  -0.0152 -0.0698 \n 6  2.45  -1.19    0.357  \n 7 -1.43  -0.0193 -0.0694 \n 8 -1.44  -0.0152 -0.0698 \n 9  7.22  -2.23    1.25   \n10 -1.44  -0.0152 -0.0698 \n# ℹ 43 more rows\n\ntidy(pca_recipe, number = 2, type = \"variance\") |&gt; \n  filter(terms == \"cumulative percent variance\") |&gt; \n  print()\n\n# A tibble: 9 × 4\n  terms                       value component id       \n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n1 cumulative percent variance  81.7         1 pca_zPW5q\n2 cumulative percent variance  92.9         2 pca_zPW5q\n3 cumulative percent variance  97.2         3 pca_zPW5q\n4 cumulative percent variance  98.8         4 pca_zPW5q\n5 cumulative percent variance  99.6         5 pca_zPW5q\n6 cumulative percent variance  99.9         6 pca_zPW5q\n7 cumulative percent variance 100.          7 pca_zPW5q\n8 cumulative percent variance 100.          8 pca_zPW5q\n9 cumulative percent variance 100           9 pca_zPW5q\n\n\nWe see that only three principal components are needed to capture 95% of the variability present in the nine predictor variables.\n\n# Visualize PCA results\npca_results %&gt;%\n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_point() +\n  theme_minimal() +\n  ggtitle(\"PCA of Protein Consumption Data\") +\n  xlab(\"Principal Component 1\") +\n  ylab(\"Principal Component 2\")"
  },
  {
    "objectID": "13_clustering.html#nba-player-styles",
    "href": "13_clustering.html#nba-player-styles",
    "title": "\n13  Clustering\n",
    "section": "\n13.1 NBA Player Styles",
    "text": "13.1 NBA Player Styles\nTo demonstrate the power of clustering in a sports context, we begin with a case study using player statistics from the NBA. Our goal is to group players based on their in-game performance during a single season, allowing us to identify archetypes such as volume scorers, defensive anchors, or well-rounded contributors. By clustering players based on key statistical measures, we can uncover patterns that may not be obvious from traditional per-game summaries.\nIn this case study, we use the hoopR package in R, which provides access to rich and detailed NBA player box score data via the ESPN API. Specifically, we’ll use data from the 2021–2022 season, focusing on player-level averages across several core performance metrics.\nWe begin by loading the necessary libraries and retrieving the data. The tidymodels ecosystem helps us structure our data processing and modeling workflow, while hoopR provides the raw game-level data.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(hoopR)\n\nThe load_nba_player_box() function fetches player box score data for the season. We then clean and summarize the data to compute per-game averages for each player. These averages form the basis of our clustering analysis.\n\nlibrary(janitor)\n\n# Get NBA player box scores for the 2021–2022 season\nnba_data &lt;- load_nba_player_box(season = 2022)\n\nThe raw box score data includes one row per player per game. Since we are interested in clustering based on overall performance style rather than single-game variation, we aggregate the data to the player level. We compute average points (PTS), assists (AST), rebounds (REB), steals (STL), blocks (BLK), turnovers (TOV), three-pointers made (FG3M), field goal attempts (FGA), and free throw attempts (FTA), and restrict our analysis to players who appeared in at least 30 games during the season to ensure we are analyzing a stable sample.\n\nplayer_stats &lt;- nba_data |&gt;\n  group_by(athlete_display_name, team_abbreviation) |&gt;\n  summarise(\n    games = n(),\n    pts = mean(points, na.rm = TRUE),\n    ast = mean(assists, na.rm = TRUE),\n    reb = mean(rebounds, na.rm = TRUE),\n    stl = mean(steals, na.rm = TRUE),\n    blk = mean(blocks, na.rm = TRUE),\n    tov = mean(turnovers, na.rm = TRUE),\n    fg3m = mean(three_point_field_goals_made, na.rm = TRUE),\n    ftm = mean(free_throws_made, na.rm = TRUE),\n    position = collapse::fmode(athlete_position_abbreviation),\n    .groups = \"drop\"\n  ) |&gt;\n  filter(games &gt;= 30)\n\nBefore performing clustering, we standardize the numeric features. This is a crucial step because the variables we are using are on different scales. For instance, points per game typically ranges from single digits to the low 30s, while steals or blocks per game rarely exceed 2 or 3. Without scaling, clustering algorithms like K-means will be unduly influenced by the variables with larger numeric ranges.\n\n# Scale the numeric columns\nscaled_stats &lt;- recipe(~., data = player_stats) |&gt; \n  step_rm(athlete_display_name, team_abbreviation, position) |&gt; \n  step_normalize(all_numeric_predictors()) |&gt; \n  prep()\n\ndat = scaled_stats |&gt; bake(new_data=NULL)\n\nThe goal of this clustering exercise is not just to segment players for the sake of grouping, but to discover meaningful player types. For example, we may find a cluster of players who attempt many field goals and score efficiently, representing primary scorers. Another group may consist of players with high assist and rebound numbers, pointing to versatile point-forwards or playmakers. Defensive specialists may emerge as a group with high block and steal rates but low scoring output."
  },
  {
    "objectID": "13_clustering.html#k-means-clustering",
    "href": "13_clustering.html#k-means-clustering",
    "title": "\n13  Clustering\n",
    "section": "\n13.2 K-means Clustering",
    "text": "13.2 K-means Clustering\nNow that we have prepared our NBA player statistics data, the next step is to apply K-means clustering, one of the most widely used and intuitive clustering algorithms. K-means aims to partition the observations into a fixed number of groups, or clusters, such that each observation belongs to the cluster with the nearest centroid—the mean position of all points within that cluster.\n\n13.2.1 How K-means Works\nK-means clustering is an iterative algorithm that seeks to minimize the total within-cluster sum of squares (WCSS), which measures the variance of the observations around the cluster centroids. Here’s how the algorithm works conceptually:\n\nChoose the number of clusters, \\(k\\), to create.\nRandomly assign initial positions for the \\(k\\) centroids.\nAssign each observation to the cluster whose centroid is closest, based on Euclidean distance.\nRecompute the centroids as the mean of the observations in each cluster.\nRepeat steps 3 and 4 until the assignments stop changing or a maximum number of iterations is reached.\n\nThe algorithm always converges, but it may converge to a local minimum depending on the starting positions. Therefore, it is common practice to run the algorithm multiple times with different random starts and retain the best result.\n\n13.2.2 Example with two features\nSuppose we only looked at a sample of 30 players with the features reb and ast.\n\n\n\n\n\nWe can see what appears to be three grouping of players:\n\n\n\n\n\nHow do we get an unsupervised method to determine these clusters instead of us visually determining them?\nWe can start by specifying that there will be three cluster and putting three points at random on the plot. We will call these points the centroids. They are the orange points in the plot below.\n\n\n\n\n\nWe now find the distance from each ppoint to the closest centroid.\n\n\n\n\n\nWe now cluster those points that share a centroid (same color in the plot below) and then update the centroids by calculating the middle of the points in the cluster.\n\n\n\n\n\nWe now repeat the process. That is, we determine which centroid is closest to each point.\n\n\n\n\n\nNext, color code the points based on the closest centroid and recalculate the position of the centroid.\n\n\n\n\n\nThis process continues until the centroids no longer moves.\nWith more than two features, the idea is still the same: calculate the distance to each centroid and make clusters based on those that are closest. Then we update the centroids and repeat the process. The difference with higher number of features is that we can no longer visualize the process as we have just done here."
  },
  {
    "objectID": "13_clustering.html#choosing-the-number-of-clusters",
    "href": "13_clustering.html#choosing-the-number-of-clusters",
    "title": "\n13  Clustering\n",
    "section": "\n13.3 Choosing the Number of Clusters",
    "text": "13.3 Choosing the Number of Clusters\nOne of the most important decisions in K-means clustering is selecting an appropriate value for \\(k\\), the number of clusters. Too few clusters can lead to overly broad groupings that obscure important differences, while too many clusters may produce noise or overfit to small nuances in the data.\nA commonly used heuristic is the Elbow Method, which plots the total WCSS for different values of \\(k\\). The idea is to choose the number of clusters at which the rate of decrease in WCSS sharply slows down—resembling an “elbow” in the plot. This point represents a good trade-off between reducing within-cluster variance and avoiding overly complex models.\nA handy plot for doing this is the fviz_nbclust plot in the factoextra package. Below, we examine the plot for all the players but with just the features reb and ast.\n\nlibrary(factoextra)\nfviz_nbclust(dat |&gt; select(reb, ast) , kmeans, method = \"wss\")\n\n\n\n\nHere we see the decrees in WCSS starts to level off at \\(k=5\\). This suggests that a five-cluster solution captures most of the structure in the data without unnecessary complexity."
  },
  {
    "objectID": "13_clustering.html#applying-k-means-to-nba-player-data",
    "href": "13_clustering.html#applying-k-means-to-nba-player-data",
    "title": "\n13  Clustering\n",
    "section": "\n13.4 Applying K-means to NBA Player Data",
    "text": "13.4 Applying K-means to NBA Player Data\nLet’s first determine the number of clusters we should use when using all 9 features.\n\nfviz_nbclust(dat, kmeans, method = \"wss\", k.max = 20)\n\n\n\n\nIn this situation, we see the number of knots is not easily chosen by the elbow method. The decrease in WCSS is consistent, for the most part, throughout the values of k.\nSince there are traditionally five positions on a basketball team, let’s go with \\(k=5\\).\nOnce we have selected the number of clusters, we fit the K-means model using the kmeans() function in base R. We use the nstart = 25 argument to perform the clustering 25 times with different initial centroids and keep the best solution based on the total WCSS.\n\nkmeans_fit &lt;- kmeans(dat, centers = 5, nstart = 25)\n\nThe kmeans_fit object contains several components, including the cluster assignments for each observation, the coordinates of the centroids, and the total within-cluster sum of squares. We add the cluster assignments to our original player-level dataset for further analysis.\n\nplayer_stats &lt;- player_stats |&gt;\n  mutate(cluster = factor(kmeans_fit$cluster))"
  },
  {
    "objectID": "13_clustering.html#interpreting-the-clusters",
    "href": "13_clustering.html#interpreting-the-clusters",
    "title": "\n13  Clustering\n",
    "section": "\n13.5 Interpreting the Clusters",
    "text": "13.5 Interpreting the Clusters\nThe most meaningful part of a clustering analysis is the interpretation of the resulting groups. To do this, we examine the average values of the original variables within each cluster. This helps us understand what defines each group.\n\nplayer_stats |&gt;\n  group_by(cluster) |&gt;\n  summarise(across(pts:ftm, mean, na.rm = TRUE))\n\n# A tibble: 5 × 9\n  cluster   pts   ast   reb   stl   blk   tov  fg3m   ftm\n  &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1       13.1  2.78   3.82 0.893 0.361 1.40  1.91  1.81 \n2 2       22.2  6.28   6.39 1.21  0.516 3.03  2.10  4.32 \n3 3        4.36 0.930  2.41 0.350 0.247 0.568 0.544 0.573\n4 4       11.9  1.87   8.03 0.758 1.20  1.42  0.537 1.96 \n5 5        6.05 1.27   2.95 0.535 0.310 0.704 0.705 0.836\n\n\nThis summary can be used to determine similar players in each cluster. Let’s look at the distribution of positions for the second cluster which has the highest aveage points and assists.\n\nplayer_stats |&gt; filter(cluster == 2) |&gt; \n  select(position) |&gt; \n  table()\n\nposition\n C  F PF PG SF SG \n 3  1  5 19  7 13 \n\n\nMost of these players in cluster 2 are guards. Let’s look at the players in this cluster.\n\nplayer_stats |&gt; \n  filter(cluster==2) |&gt; \n  select(athlete_display_name)\n\n# A tibble: 48 × 1\n   athlete_display_name\n   &lt;chr&gt;               \n 1 Anthony Edwards     \n 2 Bradley Beal        \n 3 Brandon Ingram      \n 4 CJ McCollum         \n 5 Cade Cunningham     \n 6 Chris Paul          \n 7 Cole Anthony        \n 8 D'Angelo Russell    \n 9 Darius Garland      \n10 De'Aaron Fox        \n# ℹ 38 more rows"
  },
  {
    "objectID": "13_clustering.html#visualizing-the-results",
    "href": "13_clustering.html#visualizing-the-results",
    "title": "\n13  Clustering\n",
    "section": "\n13.6 Visualizing the Results",
    "text": "13.6 Visualizing the Results\nTo help interpret the clusters visually, we can use the fviz_cluster() function, which projects the high-dimensional data onto two principal components and shows the cluster memberships in two dimensions.\n\nfviz_cluster(list(data = dat, cluster = kmeans_fit$cluster))\n\n\n\n\nWhile this visualization simplifies the data into two dimensions, it still provides a helpful overview of how well the clusters are separated and whether there is any substantial overlap between groups. Large, well-separated clusters suggest that the statistical profiles of the players are meaningfully different."
  },
  {
    "objectID": "13_clustering.html#hierarchical-clustering",
    "href": "13_clustering.html#hierarchical-clustering",
    "title": "\n13  Clustering\n",
    "section": "\n13.7 Hierarchical Clustering",
    "text": "13.7 Hierarchical Clustering\nWhile K-means clustering is powerful and widely used, it requires the analyst to choose the number of clusters in advance and is most effective when clusters are roughly spherical and of similar size. An alternative technique that overcomes some of these limitations is hierarchical clustering, which builds a tree-like structure of nested clusters. This method provides a flexible and informative view of the relationships among observations and is particularly helpful when we want to explore the data at multiple levels of granularity.\n\n13.7.1 The Basic Idea\nHierarchical clustering operates by computing a measure of dissimilarity (or distance) between each pair of observations, then successively merging (or, in some cases, splitting) observations and clusters based on that measure. The result is a dendrogram—a tree diagram that illustrates how observations group together.\nThere are two primary types of hierarchical clustering:\n\n\nAgglomerative clustering, which begins with each observation in its own cluster and repeatedly merges the two closest clusters.\n\nDivisive clustering, which starts with all observations in a single cluster and successively splits them.\n\nIn practice, agglomerative clustering is far more common and is the default approach in most statistical software.\n\n13.7.2 Distance Metrics and Linkage Methods\nHierarchical clustering requires two key decisions: how to measure the distance between observations, and how to define the distance between clusters.\nThe most common distance metric is Euclidean distance, which measures the straight-line distance between two points in multidimensional space. Other options include Manhattan distance and cosine similarity, though Euclidean is usually sufficient for standardized numerical data.\nOnce distances between individual observations are calculated, we must decide how to compute the distance between clusters. This is known as the linkage method, and several options are available:\n\n\nSingle linkage: the shortest distance between any two points in the two clusters.\n\nComplete linkage: the greatest distance between any two points.\n\nAverage linkage: the average of all pairwise distances.\n\nWard’s method: minimizes the total within-cluster variance and tends to produce compact, spherical clusters (similar in spirit to K-means).\n\nFor our analysis of NBA player statistics, we will use Ward’s method, which is generally well-suited for quantitative data and tends to create clusters of similar size.\n\n13.7.3 Applying Hierarchical Clustering to NBA Player Data\nWe begin by computing the distance matrix and fitting the hierarchical clustering model.\n\n# Remove player names and compute Euclidean distances\ndist_matrix &lt;- dist(dat )\n\n# Perform hierarchical clustering using Ward's method\nhc &lt;- hclust(dist_matrix, method = \"ward.D2\")\n\nThe hclust() function returns an object containing the hierarchy of merges, which we can visualize using a dendrogram.\n\nfviz_dend(hc, k = 5, rect = TRUE, labels_track_height = 3)\n\n\n\n\nIn the dendrogram, each leaf node represents an individual player. As we move up the tree, branches merge into larger clusters based on their similarity. The height at which two clusters are merged corresponds to the dissimilarity between them.\nWe can use this tree to cut the data into any number of clusters by drawing a horizontal line across the dendrogram. For example, if we cut the tree at \\(k = 5\\), we obtain four clusters similar to our K-means example.\n\ncluster_assignments &lt;- cutree(hc, k = 5)\n\n# Add cluster labels to the player data\nplayer_stats$hc_cluster &lt;- factor(cluster_assignments)\n\n\n13.7.4 Interpreting and Comparing Results\nOnce we have assigned players to clusters using hierarchical clustering, we can interpret the groups in the same way as we did with K-means. For example, we might summarize the mean statistics for each cluster:\n\nplayer_stats |&gt;\n  group_by(hc_cluster) |&gt;\n  summarise(across(pts:ftm, mean, na.rm = TRUE))\n\n# A tibble: 5 × 9\n  hc_cluster   pts   ast   reb   stl   blk   tov  fg3m   ftm\n  &lt;fct&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1           7.06  1.46  3.14 0.583 0.336 0.792 0.878 0.957\n2 2           4.96  1.03  2.63 0.388 0.278 0.619 0.572 0.671\n3 3          13.1   2.10  8.55 0.804 1.21  1.58  0.644 2.23 \n4 4          14.5   2.84  3.99 0.876 0.341 1.47  2.12  2.06 \n5 5          20.3   6.10  5.81 1.26  0.518 2.86  2.04  3.82 \n\n\nThis output allows us to describe the average player profile for each cluster. It’s important to note that while hierarchical clustering and K-means may yield similar types of player groupings, they do not always agree. For example, hierarchical clustering may detect smaller subgroups or more gradual transitions between player types, since it is not constrained to fixed-size or spherical clusters.\nWe can also compare the cluster assignments from K-means and hierarchical clustering directly:\n\ntable(KMeans = player_stats$cluster, Hierarchical = player_stats$hc_cluster)\n\n      Hierarchical\nKMeans   1   2   3   4   5\n     1  31   4   1  65  10\n     2   0   0   4   1  43\n     3   4 120   0   0   0\n     4   6   5  37   0   1\n     5 127  12   0   0   0\n\n\nThis contingency table shows how much overlap exists between the two methods. A high degree of agreement suggests that the player groupings are robust and meaningful, while major differences may indicate that the choice of clustering method influences the interpretation.\n\n13.7.5 Advantages and Limitations\nOne key advantage of hierarchical clustering is that it provides a complete picture of how observations relate to one another at all levels of similarity. This is particularly useful in exploratory settings where the number of natural groupings is unclear. It also does not require the analyst to pre-specify the number of clusters, unlike K-means.\nHowever, hierarchical clustering has some limitations. It can be computationally expensive for large datasets, since it must compute and store the entire distance matrix. Additionally, once a merge is made, it cannot be undone, meaning that early decisions in the hierarchy can influence the final outcome even if better options become available later. As a result, hierarchical clustering is best suited for medium-sized datasets where interpretability is a primary concern."
  },
  {
    "objectID": "13_clustering.html#comparing-k-means-and-hierarchical-clustering",
    "href": "13_clustering.html#comparing-k-means-and-hierarchical-clustering",
    "title": "\n13  Clustering\n",
    "section": "\n13.8 Comparing K-means and Hierarchical Clustering",
    "text": "13.8 Comparing K-means and Hierarchical Clustering\nClustering is often an exploratory process, and choosing the “best” algorithm depends on both the structure of the data and the goals of the analysis. In this section, we compare the results of K-means and hierarchical clustering applied to our NBA player statistics dataset. Our goal is not only to assess which algorithm performs better, but also to explore how the insights generated by each method can differ in practical and strategic ways.\n\n13.8.1 Conceptual Differences\nBefore comparing the results quantitatively, it’s useful to revisit the conceptual differences between the two approaches.\nK-means is a partitioning method. It assumes that the data are divisible into a fixed number of clusters and tries to optimize the grouping by minimizing the total within-cluster variance. The clusters produced by K-means tend to be compact and evenly sized, which works well when the true groups are spherical and well-separated. However, the algorithm requires us to specify the number of clusters in advance and is sensitive to the initial placement of centroids.\nIn contrast, hierarchical clustering is a connectivity-based method. It does not require a predetermined number of clusters and instead builds a full hierarchy that shows relationships among observations at all levels of similarity. It can reveal nested or irregular groupings, and its results are typically presented as a dendrogram. This method offers more interpretive flexibility but can be computationally intensive and less effective when the dataset contains many noise points or overlapping clusters.\n\n13.8.2 Comparing Cluster Assignments\nTo see how the two methods compare on our NBA player dataset, we can tabulate the cluster assignments for each player. Recall that we created a cluster variable from K-means and a hc_cluster variable from hierarchical clustering.\n\ntable(KMeans = player_stats$cluster, Hierarchical = player_stats$hc_cluster)\n\n      Hierarchical\nKMeans   1   2   3   4   5\n     1  31   4   1  65  10\n     2   0   0   4   1  43\n     3   4 120   0   0   0\n     4   6   5  37   0   1\n     5 127  12   0   0   0\n\n\nThis cross-tabulation shows how many players were assigned to each pair of K-means and hierarchical clusters. Large values along the diagonal suggest strong agreement between the methods, while high off-diagonal values indicate differences. For example, if most players in K-means Cluster 1 are also in hierarchical Cluster 3, we might consider these to represent similar player types despite the differing algorithmic foundations.\nYou may find that some clusters align fairly well, particularly those representing extreme player types—such as high-scoring, high-usage stars or low-usage defensive specialists. On the other hand, more nuanced or hybrid players may be assigned to different clusters by each method depending on how the algorithm interprets the multidimensional space.\n\n13.8.3 Visual Comparison\nVisualizing the clusters produced by each method is another effective way to compare results. We can again use the fviz_cluster() function to project the data into two dimensions using principal component analysis and color-code the observations by their cluster membership.\n\nfviz_cluster(list(data = dat, cluster = kmeans_fit$cluster)) +\n  ggtitle(\"K-means Clustering of NBA Players\")\n\n\n\nfviz_dend(hc, k = 5, rect = TRUE, labels_track_height = 3) +\n  ggtitle(\"Hierarchical Clustering Dendrogram\")\n\n\n\n\nThese visualizations provide complementary views of the data. The K-means plot shows distinct, compact groups, while the dendrogram reveals the hierarchical structure—allowing us to see not only how players group together but also how those groups relate to one another.\n\n13.8.4 Statistical Comparison\nAlthough clustering is fundamentally an unsupervised technique, there are ways to assess and compare the quality of different clustering results. One popular metric is the Silhouette score, which measures how well each observation fits within its assigned cluster compared to other clusters.\nWe can compute the average silhouette width for both methods:\n\n# Silhouette score for K-means\nsil_kmeans &lt;- cluster::silhouette(kmeans_fit$cluster, dist_matrix)\nmean(sil_kmeans[, \"sil_width\"])\n\n[1] 0.2197183\n\n# Silhouette score for hierarchical clustering\nsil_hc &lt;- cluster::silhouette(cluster_assignments, dist_matrix)\nmean(sil_hc[, \"sil_width\"])\n\n[1] 0.1827008\n\n\nHigher average silhouette scores indicate better-defined clusters. While this metric should not be the sole basis for choosing a method, it can provide a helpful indication of how well the structure discovered by each algorithm matches the natural groupings in the data."
  }
]